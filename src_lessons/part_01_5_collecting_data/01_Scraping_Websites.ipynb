{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "906ab954-aa4d-4add-a06a-ebb55c0cd2eb",
   "metadata": {},
   "source": [
    "# Scraping Websites\n",
    "## Extracts data from a Website\n",
    "Alternative products:\n",
    "<ul>\n",
    "  <li><a target=\"_blank\" href=\"https://www.crummy.com/software/BeautifulSoup/\">Beautiful Soap</a></li>    \n",
    "  <li><a target=\"_blank\" href=\"https://scrapy.org/\">Scrapy</a></li>    \n",
    "  <li><a target=\"_blank\" href=\"https://www.selenium.dev/\">Selenium</a> Web application test framework</li>    \n",
    "  <li><a target=\"_blank\" href=\"https://playwright.dev/\">PlayWritght</a> Web application test framework</li>    \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02212f4a-7f77-4736-a53e-97eb6eadd076",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from llama_index.core.schema import Document\n",
    "import logging as log\n",
    "import newspaper\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fe80f49-4f45-432a-8c67-4c6159dd29cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "FORMAT_STRING = \"%(module)s.%(funcName)s():%(lineno)d %(asctime)s\\n[%(levelname)-5s] %(message)s\\n\"\n",
    "log.basicConfig(level= log.INFO, format=FORMAT_STRING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "343625f5-c128-4c0d-bc3f-2ee3c61c7132",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://docs.llamaindex.ai/en/stable/understanding\",\n",
    "    \"https://docs.llamaindex.ai/en/stable/understanding/using_llms/using_llms/\",\n",
    "    \"https://docs.llamaindex.ai/en/stable/understanding/indexing/indexing/\",\n",
    "    \"https://docs.llamaindex.ai/en/stable/understanding/querying/querying/\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fedc7a48-5dab-4981-8385-2b063c49e777",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_content= []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67a558e2-1f39-43d9-bc79-430e65d493d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1493940982.<module>():4 2025-03-24 23:14:38,280\n",
      "[INFO ] url:https://docs.llamaindex.ai/en/stable/understanding\n",
      "\n",
      "1493940982.<module>():7 2025-03-24 23:14:40,841\n",
      "[INFO ] Title:Building an LLM Application\n",
      "\n",
      "1493940982.<module>():10 2025-03-24 23:14:40,842\n",
      "[INFO ] Text:\n",
      "Building an LLM application#\n",
      "\n",
      "Welcome to Understanding LlamaIndex. This is a series of short, bite-sized tutorials on every stage of building an agentic LLM application to get you acquainted with how to use LlamaIndex before diving into more advanced and subtle strategies. If you're an experienced programmer new to LlamaIndex, this is the place to start.\n",
      "\n",
      "Key steps in building an agentic LLM application#\n",
      "\n",
      "Tip You might want to read our high-level concepts if these terms are unfamiliar.\n",
      "\n",
      "This tutorial has three main parts: Building a RAG pipeline, Building an agent, and Building Workflows, with some smaller sections before and after. Here's what to expect:\n",
      "\n",
      "Ready to dive in? Head to using LLMs.\n",
      "\n",
      "1493940982.<module>():4 2025-03-24 23:14:40,844\n",
      "[INFO ] url:https://docs.llamaindex.ai/en/stable/understanding/using_llms/using_llms/\n",
      "\n",
      "1493940982.<module>():7 2025-03-24 23:14:42,005\n",
      "[INFO ] Title:Using LLMs\n",
      "\n",
      "1493940982.<module>():10 2025-03-24 23:14:42,005\n",
      "[INFO ] Text:\n",
      "Using LLMs#\n",
      "\n",
      "Tip For a list of our supported LLMs and a comparison of their functionality, check out our LLM module guide.\n",
      "\n",
      "One of the first steps when building an LLM-based application is which LLM to use; they have different strengths and price points and you may wish to use more than one.\n",
      "\n",
      "LlamaIndex provides a single interface to a large number of different LLMs. Using an LLM can be as simple as installing the appropriate integration:\n",
      "\n",
      "pip install llama-index-llms-openai\n",
      "\n",
      "And then calling it in a one-liner:\n",
      "\n",
      "from llama_index.llms.openai import OpenAI response = OpenAI () . complete ( \"William Shakespeare is \" ) print ( response )\n",
      "\n",
      "Note that this requires an API key called OPENAI_API_KEY in your environment; see the starter tutorial for more details.\n",
      "\n",
      "complete is also available as an async method, acomplete .\n",
      "\n",
      "You can also get a streaming response by calling stream_complete , which returns a generator that yields tokens as they are produced:\n",
      "\n",
      "handle = OpenAI().stream_complete(\"William Shakespeare is \") for token in handle: print(token.delta, end=\"\", flush=True)\n",
      "\n",
      "stream_complete is also available as an async method, astream_complete .\n",
      "\n",
      "Chat interface#\n",
      "\n",
      "The LLM class also implements a chat method, which allows you to have more sophisticated interactions:\n",
      "\n",
      "messages = [ ChatMessage ( role = \"system\" , content = \"You are a helpful assistant.\" ), ChatMessage ( role = \"user\" , content = \"Tell me a joke.\" ), ] chat_response = llm . chat ( messages )\n",
      "\n",
      "stream_chat and astream_chat are also available.\n",
      "\n",
      "Specifying models#\n",
      "\n",
      "Many LLM integrations provide more than one model. You can specify a model by passing the model parameter to the LLM constructor:\n",
      "\n",
      "llm = OpenAI ( model = \"gpt-4o-mini\" ) response = llm . complete ( \"Who is Laurie Voss?\" ) print ( response )\n",
      "\n",
      "Some LLMs support multi-modal chat messages. This means that you can pass in a mix of text and other modalities (images, audio, video, etc.) and the LLM will handle it.\n",
      "\n",
      "Currently, LlamaIndex supports text, images, and audio inside ChatMessages using content blocks.\n",
      "\n",
      "from llama_index.core.llms import ChatMessage , TextBlock , ImageBlock from llama_index.llms.openai import OpenAI llm = OpenAI ( model = \"gpt-4o\" ) messages = [ ChatMessage ( role = \"user\" , blocks = [ ImageBlock ( path = \"image.png\" ), TextBlock ( text = \"Describe the image in a few sentences.\" ), ], ) ] resp = llm . chat ( messages ) print ( resp . message . content )\n",
      "\n",
      "Tool Calling#\n",
      "\n",
      "Some LLMs (OpenAI, Anthropic, Gemini, Ollama, etc.) support tool calling directly over API calls -- this means tools and functions can be called without specific prompts and parsing mechanisms.\n",
      "\n",
      "from llama_index.core.tools import FunctionTool from llama_index.llms.openai import OpenAI def generate_song ( name : str , artist : str ) -> Song : \"\"\"Generates a song with provided name and artist.\"\"\" return { \"name\" : name , \"artist\" : artist } tool = FunctionTool . from_defaults ( fn = generate_song ) llm = OpenAI ( model = \"gpt-4o\" ) response = llm . predict_and_call ( [ tool ], \"Pick a random song for me\" , ) print ( str ( response ))\n",
      "\n",
      "For more details on even more advanced tool calling, check out the in-depth guide using OpenAI. The same approaches work for any LLM that supports tools/functions (e.g. Anthropic, Gemini, Ollama, etc.).\n",
      "\n",
      "You can learn more about tools and agents in the tools guide.\n",
      "\n",
      "Available LLMs#\n",
      "\n",
      "We support integrations with OpenAI, Anthropic, Mistral, DeepSeek, Hugging Face, and dozens more. Check out our module guide to LLMs for a full list, including how to run a local model.\n",
      "\n",
      "Tip A general note on privacy and LLM usage can be found on the privacy page.\n",
      "\n",
      "Using a local LLM#\n",
      "\n",
      "LlamaIndex doesn't just support hosted LLM APIs; you can also run a local model such as Meta's Llama 3 locally. For example, if you have Ollama installed and running:\n",
      "\n",
      "from llama_index.llms.ollama import Ollama llm = Ollama ( model = \"llama3.3\" , request_timeout = 60.0 )\n",
      "\n",
      "See the custom LLM's How-To for more details on using and configuring LLM models.\n",
      "\n",
      "1493940982.<module>():4 2025-03-24 23:14:42,009\n",
      "[INFO ] url:https://docs.llamaindex.ai/en/stable/understanding/indexing/indexing/\n",
      "\n",
      "1493940982.<module>():7 2025-03-24 23:14:42,928\n",
      "[INFO ] Title:Indexing & Embedding\n",
      "\n",
      "1493940982.<module>():10 2025-03-24 23:14:42,929\n",
      "[INFO ] Text:\n",
      "With your data loaded, you now have a list of Document objects (or a list of Nodes). It's time to build an Index over these objects so you can start querying them.\n",
      "\n",
      "What is an Index?#\n",
      "\n",
      "In LlamaIndex terms, an Index is a data structure composed of Document objects, designed to enable querying by an LLM. Your Index is designed to be complementary to your querying strategy.\n",
      "\n",
      "LlamaIndex offers several different index types. We'll cover the two most common here.\n",
      "\n",
      "Vector Store Index#\n",
      "\n",
      "A VectorStoreIndex is by far the most frequent type of Index you'll encounter. The Vector Store Index takes your Documents and splits them up into Nodes. It then creates vector embeddings of the text of every node, ready to be queried by an LLM.\n",
      "\n",
      "What is an embedding?#\n",
      "\n",
      "Vector embeddings are central to how LLM applications function.\n",
      "\n",
      "A vector embedding , often just called an embedding, is a numerical representation of the semantics, or meaning of your text. Two pieces of text with similar meanings will have mathematically similar embeddings, even if the actual text is quite different.\n",
      "\n",
      "This mathematical relationship enables semantic search, where a user provides query terms and LlamaIndex can locate text that is related to the meaning of the query terms rather than simple keyword matching. This is a big part of how Retrieval-Augmented Generation works, and how LLMs function in general.\n",
      "\n",
      "There are many types of embeddings, and they vary in efficiency, effectiveness and computational cost. By default LlamaIndex uses text-embedding-ada-002 , which is the default embedding used by OpenAI. If you are using different LLMs you will often want to use different embeddings.\n",
      "\n",
      "Vector Store Index embeds your documents#\n",
      "\n",
      "Vector Store Index turns all of your text into embeddings using an API from your LLM; this is what is meant when we say it \"embeds your text\". If you have a lot of text, generating embeddings can take a long time since it involves many round-trip API calls.\n",
      "\n",
      "When you want to search your embeddings, your query is itself turned into a vector embedding, and then a mathematical operation is carried out by VectorStoreIndex to rank all the embeddings by how semantically similar they are to your query.\n",
      "\n",
      "Top K Retrieval#\n",
      "\n",
      "Once the ranking is complete, VectorStoreIndex returns the most-similar embeddings as their corresponding chunks of text. The number of embeddings it returns is known as k , so the parameter controlling how many embeddings to return is known as top_k . This whole type of search is often referred to as \"top-k semantic retrieval\" for this reason.\n",
      "\n",
      "Top-k retrieval is the simplest form of querying a vector index; you will learn about more complex and subtler strategies when you read the querying section.\n",
      "\n",
      "Using Vector Store Index#\n",
      "\n",
      "To use the Vector Store Index, pass it the list of Documents you created during the loading stage:\n",
      "\n",
      "from llama_index.core import VectorStoreIndex index = VectorStoreIndex . from_documents ( documents )\n",
      "\n",
      "Tip from_documents also takes an optional argument show_progress . Set it to True to display a progress bar during index construction.\n",
      "\n",
      "You can also choose to build an index over a list of Node objects directly:\n",
      "\n",
      "from llama_index.core import VectorStoreIndex index = VectorStoreIndex ( nodes )\n",
      "\n",
      "With your text indexed, it is now technically ready for querying! However, embedding all your text can be time-consuming and, if you are using a hosted LLM, it can also be expensive. To save time and money you will want to store your embeddings first.\n",
      "\n",
      "Summary Index#\n",
      "\n",
      "A Summary Index is a simpler form of Index best suited to queries where, as the name suggests, you are trying to generate a summary of the text in your Documents. It simply stores all of the Documents and returns all of them to your query engine.\n",
      "\n",
      "Further Reading#\n",
      "\n",
      "If your data is a set of interconnected concepts (in computer science terms, a \"graph\") then you may be interested in our knowledge graph index.\n",
      "\n",
      "1493940982.<module>():4 2025-03-24 23:14:42,933\n",
      "[INFO ] url:https://docs.llamaindex.ai/en/stable/understanding/querying/querying/\n",
      "\n",
      "1493940982.<module>():7 2025-03-24 23:14:43,984\n",
      "[INFO ] Title:LlamaIndex\n",
      "\n",
      "1493940982.<module>():10 2025-03-24 23:14:43,985\n",
      "[INFO ] Text:\n",
      "Now you've loaded your data, built an index, and stored that index for later, you're ready to get to the most significant part of an LLM application: querying.\n",
      "\n",
      "At its simplest, querying is just a prompt call to an LLM: it can be a question and get an answer, or a request for summarization, or a much more complex instruction.\n",
      "\n",
      "More complex querying could involve repeated/chained prompt + LLM calls, or even a reasoning loop across multiple components.\n",
      "\n",
      "Getting started#\n",
      "\n",
      "The basis of all querying is the QueryEngine . The simplest way to get a QueryEngine is to get your index to create one for you, like this:\n",
      "\n",
      "query_engine = index . as_query_engine () response = query_engine . query ( \"Write an email to the user given their background information.\" ) print ( response )\n",
      "\n",
      "Stages of querying#\n",
      "\n",
      "However, there is more to querying than initially meets the eye. Querying consists of three distinct stages:\n",
      "\n",
      "Retrieval is when you find and return the most relevant documents for your query from your Index . As previously discussed in indexing, the most common type of retrieval is \"top-k\" semantic retrieval, but there are many other retrieval strategies.\n",
      "\n",
      "is when you find and return the most relevant documents for your query from your . As previously discussed in indexing, the most common type of retrieval is \"top-k\" semantic retrieval, but there are many other retrieval strategies. Postprocessing is when the Node s retrieved are optionally reranked, transformed, or filtered, for instance by requiring that they have specific metadata such as keywords attached.\n",
      "\n",
      "is when the s retrieved are optionally reranked, transformed, or filtered, for instance by requiring that they have specific metadata such as keywords attached. Response synthesis is when your query, your most-relevant data and your prompt are combined and sent to your LLM to return a response.\n",
      "\n",
      "Tip You can find out about how to attach metadata to documents and nodes.\n",
      "\n",
      "Customizing the stages of querying#\n",
      "\n",
      "LlamaIndex features a low-level composition API that gives you granular control over your querying.\n",
      "\n",
      "In this example, we customize our retriever to use a different number for top_k and add a post-processing step that requires that the retrieved nodes reach a minimum similarity score to be included. This would give you a lot of data when you have relevant results but potentially no data if you have nothing relevant.\n",
      "\n",
      "from llama_index.core import VectorStoreIndex , get_response_synthesizer from llama_index.core.retrievers import VectorIndexRetriever from llama_index.core.query_engine import RetrieverQueryEngine from llama_index.core.postprocessor import SimilarityPostprocessor # build index index = VectorStoreIndex . from_documents ( documents ) # configure retriever retriever = VectorIndexRetriever ( index = index , similarity_top_k = 10 , ) # configure response synthesizer response_synthesizer = get_response_synthesizer () # assemble query engine query_engine = RetrieverQueryEngine ( retriever = retriever , response_synthesizer = response_synthesizer , node_postprocessors = [ SimilarityPostprocessor ( similarity_cutoff = 0.7 )], ) # query response = query_engine . query ( \"What did the author do growing up?\" ) print ( response )\n",
      "\n",
      "You can also add your own retrieval, response synthesis, and overall query logic, by implementing the corresponding interfaces.\n",
      "\n",
      "For a full list of implemented components and the supported configurations, check out our reference docs.\n",
      "\n",
      "Let's go into more detail about customizing each step:\n",
      "\n",
      "Configuring retriever#\n",
      "\n",
      "retriever = VectorIndexRetriever ( index = index , similarity_top_k = 10 , )\n",
      "\n",
      "There are a huge variety of retrievers that you can learn about in our module guide on retrievers.\n",
      "\n",
      "Configuring node postprocessors#\n",
      "\n",
      "We support advanced Node filtering and augmentation that can further improve the relevancy of the retrieved Node objects. This can help reduce the time/number of LLM calls/cost or improve response quality.\n",
      "\n",
      "For example:\n",
      "\n",
      "KeywordNodePostprocessor : filters nodes by required_keywords and exclude_keywords .\n",
      "\n",
      ": filters nodes by and . SimilarityPostprocessor : filters nodes by setting a threshold on the similarity score (thus only supported by embedding-based retrievers)\n",
      "\n",
      ": filters nodes by setting a threshold on the similarity score (thus only supported by embedding-based retrievers) PrevNextNodePostprocessor : augments retrieved Node objects with additional relevant context based on Node relationships.\n",
      "\n",
      "The full list of node postprocessors is documented in the Node Postprocessor Reference.\n",
      "\n",
      "To configure the desired node postprocessors:\n",
      "\n",
      "node_postprocessors = [ KeywordNodePostprocessor ( required_keywords = [ \"Combinator\" ], exclude_keywords = [ \"Italy\" ] ) ] query_engine = RetrieverQueryEngine . from_args ( retriever , node_postprocessors = node_postprocessors ) response = query_engine . query ( \"What did the author do growing up?\" )\n",
      "\n",
      "Configuring response synthesis#\n",
      "\n",
      "After a retriever fetches relevant nodes, a BaseSynthesizer synthesizes the final response by combining the information.\n",
      "\n",
      "You can configure it via\n",
      "\n",
      "query_engine = RetrieverQueryEngine . from_args ( retriever , response_mode = response_mode )\n",
      "\n",
      "Right now, we support the following options:\n",
      "\n",
      "default : \"create and refine\" an answer by sequentially going through each retrieved Node ; This makes a separate LLM call per Node. Good for more detailed answers.\n",
      "\n",
      ": \"create and refine\" an answer by sequentially going through each retrieved ; This makes a separate LLM call per Node. Good for more detailed answers. compact : \"compact\" the prompt during each LLM call by stuffing as many Node text chunks that can fit within the maximum prompt size. If there are too many chunks to stuff in one prompt, \"create and refine\" an answer by going through multiple prompts.\n",
      "\n",
      ": \"compact\" the prompt during each LLM call by stuffing as many text chunks that can fit within the maximum prompt size. If there are too many chunks to stuff in one prompt, \"create and refine\" an answer by going through multiple prompts. tree_summarize : Given a set of Node objects and the query, recursively construct a tree and return the root node as the response. Good for summarization purposes.\n",
      "\n",
      ": Given a set of objects and the query, recursively construct a tree and return the root node as the response. Good for summarization purposes. no_text : Only runs the retriever to fetch the nodes that would have been sent to the LLM, without actually sending them. Then can be inspected by checking response.source_nodes . The response object is covered in more detail in Section 5.\n",
      "\n",
      ": Only runs the retriever to fetch the nodes that would have been sent to the LLM, without actually sending them. Then can be inspected by checking . The response object is covered in more detail in Section 5. accumulate : Given a set of Node objects and the query, apply the query to each Node text chunk while accumulating the responses into an array. Returns a concatenated string of all responses. Good for when you need to run the same query separately against each text chunk.\n",
      "\n",
      "Structured Outputs#\n",
      "\n",
      "You may want to ensure your output is structured. See our Query Engines + Pydantic Outputs to see how to extract a Pydantic object from a query engine class.\n",
      "\n",
      "Also make sure to check out our entire Structured Outputs guide.\n",
      "\n",
      "Creating your own Query Workflow#\n",
      "\n",
      "If you want to design complex query flows, you can compose your own query workflow across many different modules, from prompts/LLMs/output parsers to retrievers to response synthesizers to your own custom components.\n",
      "\n",
      "Take a look at our Workflow Guide for more details.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'url': 'https://docs.llamaindex.ai/en/stable/understanding', 'title': 'Building an LLM Application', 'text': \"Building an LLM application#\\n\\nWelcome to Understanding LlamaIndex. This is a series of short, bite-sized tutorials on every stage of building an agentic LLM application to get you acquainted with how to use LlamaIndex before diving into more advanced and subtle strategies. If you're an experienced programmer new to LlamaIndex, this is the place to start.\\n\\nKey steps in building an agentic LLM application#\\n\\nTip You might want to read our high-level concepts if these terms are unfamiliar.\\n\\nThis tutorial has three main parts: Building a RAG pipeline, Building an agent, and Building Workflows, with some smaller sections before and after. Here's what to expect:\\n\\nReady to dive in? Head to using LLMs.\"}\n"
     ]
    }
   ],
   "source": [
    "for url in urls:\n",
    "    try:\n",
    "        article = newspaper.Article(url)\n",
    "        log.info(f'url:{url}')\n",
    "        article.download() # Retrieves the content.\n",
    "        article.parse() # Extracts the content\n",
    "        log.info(f'Title:{article.title}')\n",
    "        if len(article.text) > 0:\n",
    "            pages_content.append({ \"url\": url, \"title\": article.title, \"text\": article.text })\n",
    "            log.info(f'Text:\\n{article.text}')\n",
    "    except:\n",
    "        continue\n",
    "\t\n",
    "print(pages_content[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3861665-de18-4d84-9529-6cb64ca0308b",
   "metadata": {},
   "source": [
    "## Transforms the extracted text into a LlamaIndex document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "099c3a46-2f43-454d-8b21-8a60750589d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [Document(text=row['text'], metadata={\"title\": row['title'], \"url\": row['url']}) for row in pages_content]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adca1f8-06a2-4205-b3f2-3d2ffa58a86d",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "### Keyword filtering\n",
    "Removes documents that do not contain any keywords in a given set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9f1ccc6-d493-4fbb-8c90-16d825f43289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def keyword_filter(documents, keywords):\n",
    "    filtered_docs = []\n",
    "    for doc in documents:\n",
    "        if any(keyword.lower() in doc.text.lower() for keyword in keywords):\n",
    "            filtered_docs.append(doc)\n",
    "    return filtered_docs\n",
    "\n",
    "ai_keywords = [\"artificial intelligence\", \"machine learning\", \"neural networks\", \"deep learning\",'LlamaIndex','llm','rag']\n",
    "filtered_documents = keyword_filter(documents, ai_keywords)\n",
    "len(filtered_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0659e4bc-c687-42a5-83d7-9af2cbf465a9",
   "metadata": {},
   "source": [
    "### Remove Boilerplate content and clean the HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b69c4dcb-b09b-492b-ba06-d1455d25ab80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Remove script and style elements\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.decompose()\n",
    "    \n",
    "    # Extract text from remaining tags\n",
    "    text = soup.get_text()\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a45af304-d300-49e4-a130-f1098e5b8fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_documents = [Document(text=clean_html(doc.text), metadata=doc.metadata) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd17e831-e196-42c1-93c5-b96261c39afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title:Building an LLM Application\n",
      "Title:Using LLMs\n",
      "Title:Indexing & Embedding\n",
      "Title:LlamaIndex\n"
     ]
    }
   ],
   "source": [
    "for doc in cleaned_documents:\n",
    "    print(f\"Title:{doc.metadata['title']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd991f6-f942-4285-a3ca-c70fd19ae085",
   "metadata": {},
   "source": [
    "### Truncating long documents\n",
    "Truncates documents that exceed a given number of tokens.  The document is tokenized into words separated by whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6a82ce1-d3cc-4865-899a-06ddaa824a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_document(doc, max_tokens=1000):\n",
    "    tokens = doc.text.split()\n",
    "    if len(tokens) > max_tokens:\n",
    "        truncated_text = ' '.join(tokens[:max_tokens])\n",
    "        return Document(text=truncated_text, metadata=doc.metadata)\n",
    "    return doc\n",
    "\n",
    "truncated_documents = [truncate_document(doc) for doc in cleaned_documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6fe6d12-2323-4774-b55a-1badc6f4ce5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title:Building an LLM Application\n",
      "Building an LLM application#\n",
      "Welcome to Understanding LlamaIndex. This is a series of short, bite-sized tutorials on every stage of building an agentic LLM application to get you acquainted with how to use LlamaIndex before diving into more advanced and subtle strategies. If you're an experienced programmer new to LlamaIndex, this is the place to start.\n",
      "Key steps in building an agentic LLM application#\n",
      "Tip You might want to read our high-level concepts if these terms are unfamiliar.\n",
      "This tutorial has three main parts: Building a RAG pipeline, Building an agent, and Building Workflows, with some smaller sections before and after. Here's what to expect:\n",
      "Ready to dive in? Head to using LLMs.\n",
      "\n",
      "Title:Using LLMs\n",
      "Using LLMs#\n",
      "Tip For a list of our supported LLMs and a comparison of their functionality, check out our LLM module guide.\n",
      "One of the first steps when building an LLM-based application is which LLM to use; they have different strengths and price points and you may wish to use more than one.\n",
      "LlamaIndex provides a single interface to a large number of different LLMs. Using an LLM can be as simple as installing the appropriate integration:\n",
      "pip install llama-index-llms-openai\n",
      "And then calling it in a one-liner:\n",
      "from llama_index.llms.openai import OpenAI response = OpenAI () . complete ( \"William Shakespeare is \" ) print ( response )\n",
      "Note that this requires an API key called OPENAI_API_KEY in your environment; see the starter tutorial for more details.\n",
      "complete is also available as an async method, acomplete .\n",
      "You can also get a streaming response by calling stream_complete , which returns a generator that yields tokens as they are produced:\n",
      "handle = OpenAI().stream_complete(\"William Shakespeare is \") for token in handle: print(token.delta, end=\"\", flush=True)\n",
      "stream_complete is also available as an async method, astream_complete .\n",
      "Chat interface#\n",
      "The LLM class also implements a chat method, which allows you to have more sophisticated interactions:\n",
      "messages = [ ChatMessage ( role = \"system\" , content = \"You are a helpful assistant.\" ), ChatMessage ( role = \"user\" , content = \"Tell me a joke.\" ), ] chat_response = llm . chat ( messages )\n",
      "stream_chat and astream_chat are also available.\n",
      "Specifying models#\n",
      "Many LLM integrations provide more than one model. You can specify a model by passing the model parameter to the LLM constructor:\n",
      "llm = OpenAI ( model = \"gpt-4o-mini\" ) response = llm . complete ( \"Who is Laurie Voss?\" ) print ( response )\n",
      "Some LLMs support multi-modal chat messages. This means that you can pass in a mix of text and other modalities (images, audio, video, etc.) and the LLM will handle it.\n",
      "Currently, LlamaIndex supports text, images, and audio inside ChatMessages using content blocks.\n",
      "from llama_index.core.llms import ChatMessage , TextBlock , ImageBlock from llama_index.llms.openai import OpenAI llm = OpenAI ( model = \"gpt-4o\" ) messages = [ ChatMessage ( role = \"user\" , blocks = [ ImageBlock ( path = \"image.png\" ), TextBlock ( text = \"Describe the image in a few sentences.\" ), ], ) ] resp = llm . chat ( messages ) print ( resp . message . content )\n",
      "Tool Calling#\n",
      "Some LLMs (OpenAI, Anthropic, Gemini, Ollama, etc.) support tool calling directly over API calls -- this means tools and functions can be called without specific prompts and parsing mechanisms.\n",
      "from llama_index.core.tools import FunctionTool from llama_index.llms.openai import OpenAI def generate_song ( name : str , artist : str ) -> Song : \"\"\"Generates a song with provided name and artist.\"\"\" return { \"name\" : name , \"artist\" : artist } tool = FunctionTool . from_defaults ( fn = generate_song ) llm = OpenAI ( model = \"gpt-4o\" ) response = llm . predict_and_call ( [ tool ], \"Pick a random song for me\" , ) print ( str ( response ))\n",
      "For more details on even more advanced tool calling, check out the in-depth guide using OpenAI. The same approaches work for any LLM that supports tools/functions (e.g. Anthropic, Gemini, Ollama, etc.).\n",
      "You can learn more about tools and agents in the tools guide.\n",
      "Available LLMs#\n",
      "We support integrations with OpenAI, Anthropic, Mistral, DeepSeek, Hugging Face, and dozens more. Check out our module guide to LLMs for a full list, including how to run a local model.\n",
      "Tip A general note on privacy and LLM usage can be found on the privacy page.\n",
      "Using a local LLM#\n",
      "LlamaIndex doesn't just support hosted LLM APIs; you can also run a local model such as Meta's Llama 3 locally. For example, if you have Ollama installed and running:\n",
      "from llama_index.llms.ollama import Ollama llm = Ollama ( model = \"llama3.3\" , request_timeout = 60.0 )\n",
      "See the custom LLM's How-To for more details on using and configuring LLM models.\n",
      "\n",
      "Title:Indexing & Embedding\n",
      "With your data loaded, you now have a list of Document objects (or a list of Nodes). It's time to build an Index over these objects so you can start querying them.\n",
      "What is an Index?#\n",
      "In LlamaIndex terms, an Index is a data structure composed of Document objects, designed to enable querying by an LLM. Your Index is designed to be complementary to your querying strategy.\n",
      "LlamaIndex offers several different index types. We'll cover the two most common here.\n",
      "Vector Store Index#\n",
      "A VectorStoreIndex is by far the most frequent type of Index you'll encounter. The Vector Store Index takes your Documents and splits them up into Nodes. It then creates vector embeddings of the text of every node, ready to be queried by an LLM.\n",
      "What is an embedding?#\n",
      "Vector embeddings are central to how LLM applications function.\n",
      "A vector embedding , often just called an embedding, is a numerical representation of the semantics, or meaning of your text. Two pieces of text with similar meanings will have mathematically similar embeddings, even if the actual text is quite different.\n",
      "This mathematical relationship enables semantic search, where a user provides query terms and LlamaIndex can locate text that is related to the meaning of the query terms rather than simple keyword matching. This is a big part of how Retrieval-Augmented Generation works, and how LLMs function in general.\n",
      "There are many types of embeddings, and they vary in efficiency, effectiveness and computational cost. By default LlamaIndex uses text-embedding-ada-002 , which is the default embedding used by OpenAI. If you are using different LLMs you will often want to use different embeddings.\n",
      "Vector Store Index embeds your documents#\n",
      "Vector Store Index turns all of your text into embeddings using an API from your LLM; this is what is meant when we say it \"embeds your text\". If you have a lot of text, generating embeddings can take a long time since it involves many round-trip API calls.\n",
      "When you want to search your embeddings, your query is itself turned into a vector embedding, and then a mathematical operation is carried out by VectorStoreIndex to rank all the embeddings by how semantically similar they are to your query.\n",
      "Top K Retrieval#\n",
      "Once the ranking is complete, VectorStoreIndex returns the most-similar embeddings as their corresponding chunks of text. The number of embeddings it returns is known as k , so the parameter controlling how many embeddings to return is known as top_k . This whole type of search is often referred to as \"top-k semantic retrieval\" for this reason.\n",
      "Top-k retrieval is the simplest form of querying a vector index; you will learn about more complex and subtler strategies when you read the querying section.\n",
      "Using Vector Store Index#\n",
      "To use the Vector Store Index, pass it the list of Documents you created during the loading stage:\n",
      "from llama_index.core import VectorStoreIndex index = VectorStoreIndex . from_documents ( documents )\n",
      "Tip from_documents also takes an optional argument show_progress . Set it to True to display a progress bar during index construction.\n",
      "You can also choose to build an index over a list of Node objects directly:\n",
      "from llama_index.core import VectorStoreIndex index = VectorStoreIndex ( nodes )\n",
      "With your text indexed, it is now technically ready for querying! However, embedding all your text can be time-consuming and, if you are using a hosted LLM, it can also be expensive. To save time and money you will want to store your embeddings first.\n",
      "Summary Index#\n",
      "A Summary Index is a simpler form of Index best suited to queries where, as the name suggests, you are trying to generate a summary of the text in your Documents. It simply stores all of the Documents and returns all of them to your query engine.\n",
      "Further Reading#\n",
      "If your data is a set of interconnected concepts (in computer science terms, a \"graph\") then you may be interested in our knowledge graph index.\n",
      "\n",
      "Title:LlamaIndex\n",
      "Now you've loaded your data, built an index, and stored that index for later, you're ready to get to the most significant part of an LLM application: querying. At its simplest, querying is just a prompt call to an LLM: it can be a question and get an answer, or a request for summarization, or a much more complex instruction. More complex querying could involve repeated/chained prompt + LLM calls, or even a reasoning loop across multiple components. Getting started# The basis of all querying is the QueryEngine . The simplest way to get a QueryEngine is to get your index to create one for you, like this: query_engine = index . as_query_engine () response = query_engine . query ( \"Write an email to the user given their background information.\" ) print ( response ) Stages of querying# However, there is more to querying than initially meets the eye. Querying consists of three distinct stages: Retrieval is when you find and return the most relevant documents for your query from your Index . As previously discussed in indexing, the most common type of retrieval is \"top-k\" semantic retrieval, but there are many other retrieval strategies. is when you find and return the most relevant documents for your query from your . As previously discussed in indexing, the most common type of retrieval is \"top-k\" semantic retrieval, but there are many other retrieval strategies. Postprocessing is when the Node s retrieved are optionally reranked, transformed, or filtered, for instance by requiring that they have specific metadata such as keywords attached. is when the s retrieved are optionally reranked, transformed, or filtered, for instance by requiring that they have specific metadata such as keywords attached. Response synthesis is when your query, your most-relevant data and your prompt are combined and sent to your LLM to return a response. Tip You can find out about how to attach metadata to documents and nodes. Customizing the stages of querying# LlamaIndex features a low-level composition API that gives you granular control over your querying. In this example, we customize our retriever to use a different number for top_k and add a post-processing step that requires that the retrieved nodes reach a minimum similarity score to be included. This would give you a lot of data when you have relevant results but potentially no data if you have nothing relevant. from llama_index.core import VectorStoreIndex , get_response_synthesizer from llama_index.core.retrievers import VectorIndexRetriever from llama_index.core.query_engine import RetrieverQueryEngine from llama_index.core.postprocessor import SimilarityPostprocessor # build index index = VectorStoreIndex . from_documents ( documents ) # configure retriever retriever = VectorIndexRetriever ( index = index , similarity_top_k = 10 , ) # configure response synthesizer response_synthesizer = get_response_synthesizer () # assemble query engine query_engine = RetrieverQueryEngine ( retriever = retriever , response_synthesizer = response_synthesizer , node_postprocessors = [ SimilarityPostprocessor ( similarity_cutoff = 0.7 )], ) # query response = query_engine . query ( \"What did the author do growing up?\" ) print ( response ) You can also add your own retrieval, response synthesis, and overall query logic, by implementing the corresponding interfaces. For a full list of implemented components and the supported configurations, check out our reference docs. Let's go into more detail about customizing each step: Configuring retriever# retriever = VectorIndexRetriever ( index = index , similarity_top_k = 10 , ) There are a huge variety of retrievers that you can learn about in our module guide on retrievers. Configuring node postprocessors# We support advanced Node filtering and augmentation that can further improve the relevancy of the retrieved Node objects. This can help reduce the time/number of LLM calls/cost or improve response quality. For example: KeywordNodePostprocessor : filters nodes by required_keywords and exclude_keywords . : filters nodes by and . SimilarityPostprocessor : filters nodes by setting a threshold on the similarity score (thus only supported by embedding-based retrievers) : filters nodes by setting a threshold on the similarity score (thus only supported by embedding-based retrievers) PrevNextNodePostprocessor : augments retrieved Node objects with additional relevant context based on Node relationships. The full list of node postprocessors is documented in the Node Postprocessor Reference. To configure the desired node postprocessors: node_postprocessors = [ KeywordNodePostprocessor ( required_keywords = [ \"Combinator\" ], exclude_keywords = [ \"Italy\" ] ) ] query_engine = RetrieverQueryEngine . from_args ( retriever , node_postprocessors = node_postprocessors ) response = query_engine . query ( \"What did the author do growing up?\" ) Configuring response synthesis# After a retriever fetches relevant nodes, a BaseSynthesizer synthesizes the final response by combining the information. You can configure it via query_engine = RetrieverQueryEngine . from_args ( retriever , response_mode = response_mode ) Right now, we support the following options: default : \"create and refine\" an answer by sequentially going through each retrieved Node ; This makes a separate LLM call per Node. Good for more detailed answers. : \"create and refine\" an answer by sequentially going through each retrieved ; This makes a separate LLM call per Node. Good for more detailed answers. compact : \"compact\" the prompt during each LLM call by stuffing as many Node text chunks that can fit within the maximum prompt size. If there are too many chunks to stuff in one prompt, \"create and refine\" an answer by going through multiple prompts. : \"compact\" the prompt during each LLM call by stuffing as many text chunks that can fit within the maximum prompt size. If there are too many chunks to stuff in one prompt, \"create and refine\" an answer by going through multiple prompts. tree_summarize : Given a set of Node objects and the query, recursively construct a tree and return the root node as the response. Good for summarization purposes. : Given a set of objects and the query, recursively construct a tree and return the root node as the response. Good for summarization purposes. no_text : Only runs the retriever to fetch the nodes that would have been sent to the LLM, without actually sending them. Then can be inspected by checking response.source_nodes . The response\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for doc in truncated_documents:\n",
    "    print(f\"Title:{doc.metadata['title']}\")\n",
    "    print(f'{doc.text}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2ea79c-c2ba-4edb-8f22-2478c70f0af2",
   "metadata": {},
   "source": [
    "## Create the RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9966542-2a47-4594-9f58-8bac4199da90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.embeddings.gemini import GeminiEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.llms.gemini import Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fb6a887b-4f42-42d2-a7a8-955192ad4c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model=\"gpt-4o-mini\",temperature=0)\n",
    "embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
    "text_splitter = SentenceSplitter(chunk_size=512, chunk_overlap=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e6848902-3ceb-4b4e-923a-edaa354284d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "Settings.text_splitter = text_splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8c341474-4545-41f2-8bd9-b62758c7b12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_client._send_single_request():1025 2025-03-24 22:58:43,390\n",
      "[INFO ] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c3462af4-e478-4399-ba65-1d83888b3619",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_client._send_single_request():1025 2025-03-24 22:59:11,638\n",
      "[INFO ] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "\n",
      "_client._send_single_request():1025 2025-03-24 22:59:14,300\n",
      "[INFO ] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A query engine is a component that facilitates the process of querying by allowing users to interact with an index to retrieve relevant information. It enables the execution of prompts, which can range from simple questions to complex instructions, and is essential for synthesizing responses based on the retrieved data. The query engine can be created from an index and is responsible for managing the stages of querying, including retrieval, postprocessing, and response synthesis.\n",
      "Node ID\t 7a14df7e-8132-4322-8dd7-9b225de2968f\n",
      "Title\t LlamaIndex\n",
      "URL\t https://docs.llamaindex.ai/en/stable/understanding/querying/querying/\n",
      "Score\t 0.5302384018558375\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "Node ID\t 31c6b250-6f78-4ab1-82b3-ae3ef0872934\n",
      "Title\t LlamaIndex\n",
      "URL\t https://docs.llamaindex.ai/en/stable/understanding/querying/querying/\n",
      "Score\t 0.4344452085265439\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n"
     ]
    }
   ],
   "source": [
    "res = query_engine.query(\"What is a query engine?\")\n",
    "\n",
    "print(res.response)\n",
    "\n",
    "# Show the retrieved nodes\n",
    "for src in res.source_nodes:\n",
    "  print(\"Node ID\\t\", src.node_id)\n",
    "  print(\"Title\\t\", src.metadata['title'])\n",
    "  print(\"URL\\t\", src.metadata['url'])\n",
    "  print(\"Score\\t\", src.score)\n",
    "  print(\"-_\"*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c68d93-3b71-4c21-a612-954a468a9712",
   "metadata": {},
   "source": [
    "### Example using Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f76aa832-1c2a-4eb5-a537-753f3fa89124",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22648/4294027278.py:2: DeprecationWarning: Call to deprecated class GeminiEmbedding. (Should use `llama-index-embeddings-google-genai` instead, using Google's latest unified SDK. See: https://docs.llamaindex.ai/en/stable/examples/embeddings/google_genai/)\n",
      "  embed_model = GeminiEmbedding(model=\"models/embedding-001\")\n"
     ]
    }
   ],
   "source": [
    "llm = Gemini(model=\"models/gemini-2.0-flash\")\n",
    "embed_model = GeminiEmbedding(model=\"models/embedding-001\")\n",
    "text_splitter = SentenceSplitter(chunk_size=512, chunk_overlap=30)\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "Settings.text_splitter = text_splitter\n",
    "\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "22a0621e-bc73-41ab-9a04-7799a28924cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A QueryEngine is the basis of all querying. You can obtain a QueryEngine by having your index create one for you.\n",
      "\n",
      "Node ID\t de14ac15-7fb4-4228-a7f9-fc9023cf203c\n",
      "Title\t LlamaIndex\n",
      "URL\t https://docs.llamaindex.ai/en/stable/understanding/querying/querying/\n",
      "Score\t 0.694896492999527\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "Node ID\t c265c99e-39e3-4f9e-85fe-9e73f2ddf8a7\n",
      "Title\t LlamaIndex\n",
      "URL\t https://docs.llamaindex.ai/en/stable/understanding/querying/querying/\n",
      "Score\t 0.6829592665283133\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n"
     ]
    }
   ],
   "source": [
    "res = query_engine.query(\"What is a query engine?\")\n",
    "\n",
    "print(res.response)\n",
    "\n",
    "# Show the retrieved nodes\n",
    "for src in res.source_nodes:\n",
    "  print(\"Node ID\\t\", src.node_id)\n",
    "  print(\"Title\\t\", src.metadata['title'])\n",
    "  print(\"URL\\t\", src.metadata['url'])\n",
    "  print(\"Score\\t\", src.score)\n",
    "  print(\"-_\"*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3051f668-1563-412b-8c67-0b57f87da6d1",
   "metadata": {},
   "source": [
    "# Another Example "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
