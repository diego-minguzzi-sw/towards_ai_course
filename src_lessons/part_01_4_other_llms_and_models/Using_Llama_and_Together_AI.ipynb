{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04fddccf-6f40-49d0-a465-d7a0f48a0854",
   "metadata": {},
   "source": [
    "# Part 1 Building RAG AI Tutor - Using other LLMs and Embedding Models\n",
    "## Using LLama 3.1 70B on Together.ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "54bc4fbc-169e-4e52-a2ed-bb13b9aa6d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document, VectorStoreIndex\n",
    "from llama_index.core.evaluation import generate_question_context_pairs\n",
    "from llama_index.core.evaluation import RetrieverEvaluator, RelevancyEvaluator, FaithfulnessEvaluator, BatchEvalRunner\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core.llms.utils import LLM\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.core.schema import MetadataMode, TextNode\n",
    "from llama_index.core.text_splitter import TokenTextSplitter\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.evaluation import EmbeddingQAFinetuneDataset\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.llms.together import TogetherLLM\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import asyncio\n",
    "import chromadb\n",
    "import csv\n",
    "import json\n",
    "import nest_asyncio\n",
    "import os\n",
    "import pprint\n",
    "import re\n",
    "import time\n",
    "import uuid\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b4a4d60-ccc8-4aa4-9773-8b162bd84752",
   "metadata": {},
   "outputs": [],
   "source": [
    "TogetherApiKeyEnvVar='TOGETHER_AI_API_KEY'\n",
    "assert TogetherApiKeyEnvVar in os.environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fcca846-6ec3-4255-9c1b-621fa67b8ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0202cd5-87ad-40e8-b80b-11c18dcfb57c",
   "metadata": {},
   "source": [
    "<b>Create a Vector Store</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cce7eaca-169f-4822-a525-b10ff4907a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorStoreName = \"mini-llama-articles\"\n",
    "chromaClient = chromadb.PersistentClient( path=vectorStoreName)\n",
    "chromaCollection = chromaClient.get_or_create_collection( vectorStoreName)\n",
    "vectorStore = ChromaVectorStore( chroma_collection=chromaCollection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d274cc2f-789c-4ce9-9730-79bbd19e3918",
   "metadata": {},
   "source": [
    "<b>Ingest documents into the Vector storage.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24e3eb29-1b57-4fa0-9882-970df10e2226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  169k  100  169k    0     0   363k      0 --:--:-- --:--:-- --:--:--  363k\n"
     ]
    }
   ],
   "source": [
    "!curl -o ./mini-llama-articles.csv \"https://raw.githubusercontent.com/AlaFalaki/tutorial_notebooks/main/data/mini-llama-articles.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09a0dabf-6f0b-40d9-945f-36b2d6fd8e61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = []\n",
    "\n",
    "with open(\"mini-llama-articles.csv\", mode=\"r\", encoding=\"utf-8\") as file: \n",
    "    csvReader = csv.reader(file)\n",
    "    for index, row in enumerate( csvReader):\n",
    "        if 0==index: \n",
    "            continue\n",
    "        rows.append(row)\n",
    "len(rows)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb4495d4-c054-4efe-9607-3c95dafee4f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = [Document(text=row[1], \n",
    "                      metadata={\"title\": row[0], \"url\": row[2], \"source_name\": row[3]}) \n",
    "             for row in rows]\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da5eef70-655f-46ac-b422-49123bb490dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "textSplitter = TokenTextSplitter( separator=\" \", chunk_size=512, chunk_overlap=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c2d6bd5-4c6f-4960-a9db-1e76d0d0bc3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4af8dbcec5eb45ce882b844aebad897b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90918bfe21e645e88f570a7d1663646c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the pipeline to apply the transformation to the documents i.e. the chunks.\n",
    "# Stores the nodes into the vector store.\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        textSplitter,\n",
    "        HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "    ],\n",
    "    vector_store=vectorStore\n",
    ")\n",
    "\n",
    "# Run the pipeline to produce the nodes.\n",
    "nodes = pipeline.run(documents=documents, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf82acc2-d264-4417-8965-cc20f4ff7970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "llama_index.core.schema.TextNode"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nodes[0]) # Each node is a TextNode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb7fd9e-4121-4e5b-826c-8f62ed72f1cc",
   "metadata": {},
   "source": [
    "<b>Creates the query engine, using the LLM and the Vector Store</B>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bfe0562f-63fd-4bec-92a9-d1de400f9402",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = TogetherLLM( model=\"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\", api_key=os.environ[TogetherApiKeyEnvVar])\n",
    "index = VectorStoreIndex.from_vector_store(vectorStore, \n",
    "                                           embed_model=\"local:BAAI/bge-small-en-v1.5\", \n",
    "                                           similarity_top_k=2)\n",
    "\n",
    "queryEngine = index.as_query_engine(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4f7eacc6-faac-4f6e-a5c4-e7d73fa6d226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama 2 is available in four different model sizes: 7 billion, 13 billion, 34 billion, and 70 billion parameters.\n"
     ]
    }
   ],
   "source": [
    "# Test the Query engine\n",
    "res = queryEngine.query(\"How many parameters LLaMA2 has?\")\n",
    "print(res.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "401e3279-1fe2-4463-a178-776eccb0dd68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node ID\\t 07f05b18-77c0-4122-aa1c-7a43d7bf772b\n",
      "Title\\t Meta's Llama 2: Revolutionizing Open Source Language Models for Commercial Use\n",
      "Score\\t 0.6191229753131267\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for src in res.source_nodes:\n",
    "    print(\"Node ID\\\\t\", src.node_id)\n",
    "    print(\"Title\\\\t\", src.metadata['title'])\n",
    "    print(\"Score\\\\t\", src.score)\n",
    "    print(\"-_\"*20)\n",
    "len(res.source_nodes)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0b13bb-9641-4bd0-92d6-ec849b9e6f59",
   "metadata": {},
   "source": [
    "<b>Evaluates the retriever.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4d305652-922a-4a9c-a328-feccd9796d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a Teacher/Professor. \n",
      "Your task is to setup questions for an upcoming examination. \n",
      "The questions should be diverse in nature across the document. \n",
      "Restrict the questions to the context information provided.\n",
      "The questions must be about the provided context.\n",
      "Do not use any other knowledge about the subject.\n",
      "Questions should be easy to understand.\n",
      "\n",
      "Context information is below:\n",
      "\n",
      "{context_str}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DEFAULT_QA_GENERATE_PROMPT_TMPL = \"\"\"\n",
    "You are a Teacher/Professor. \n",
    "Your task is to setup questions for an upcoming examination. \n",
    "The questions should be diverse in nature across the document. \n",
    "Restrict the questions to the context information provided.\n",
    "The questions must be about the provided context.\n",
    "Do not use any other knowledge about the subject.\n",
    "Questions should be easy to understand.\n",
    "\n",
    "Context information is below:\n",
    "\n",
    "{context_str}\n",
    "\"\"\"\n",
    "print(DEFAULT_QA_GENERATE_PROMPT_TMPL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a53a2089-2e70-4068-b081-196d7df4717f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myGenerateQuestionContextPairs(\n",
    "    nodes: List[TextNode],\n",
    "    llm: LLM,\n",
    "    qa_generate_prompt_tmpl: str = DEFAULT_QA_GENERATE_PROMPT_TMPL,\n",
    "    num_questions_per_chunk: int = 2,\n",
    "    request_delay: float = 2.0\n",
    ") -> EmbeddingQAFinetuneDataset:\n",
    "    \"\"\"Generate examples given a set of nodes with delays between requests.\"\"\"\n",
    "    node_dict = {\n",
    "        node.node_id: node.get_content(metadata_mode=MetadataMode.NONE)\n",
    "        for node in nodes\n",
    "    }\n",
    "\n",
    "    queries = {}\n",
    "    relevant_docs = {}\n",
    "\n",
    "    for node_id, text in tqdm(node_dict.items()):\n",
    "        query = qa_generate_prompt_tmpl.format(\n",
    "            context_str=text, num_questions_per_chunk=num_questions_per_chunk\n",
    "        )\n",
    "        response = llm.complete(query)\n",
    "\n",
    "        result = str(response).strip().split(\"\\n\")\n",
    "        questions = [\n",
    "            re.sub(r\"^\\d+[\\).\\s]\", \"\", question).strip() for question in result\n",
    "        ]\n",
    "        questions = [question for question in questions if len(question) > 0][\n",
    "            :num_questions_per_chunk\n",
    "        ]\n",
    "\n",
    "        num_questions_generated = len(questions)\n",
    "        if num_questions_generated < num_questions_per_chunk:\n",
    "            warnings.warn(\n",
    "                f\"Fewer questions generated ({num_questions_generated}) \"\n",
    "                f\"than requested ({num_questions_per_chunk}).\"\n",
    "            )\n",
    "\n",
    "        for question in questions:\n",
    "            question_id = str(uuid.uuid4())\n",
    "            queries[question_id] = question\n",
    "            relevant_docs[question_id] = [node_id]\n",
    "\n",
    "        time.sleep(request_delay)\n",
    "\n",
    "    return EmbeddingQAFinetuneDataset(\n",
    "        queries=queries, corpus=node_dict, relevant_docs=relevant_docs\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ee8149e3-eb51-4571-987c-ce2495d7245c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval dataset rag_eval_dataset_new.json does not exist: creating it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [01:32<00:00,  9.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created rag_eval_dataset_new.json.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evalDatasetFile='rag_eval_dataset_new.json'\n",
    "llmOai = OpenAI(model=\"gpt-4o-mini\")\n",
    "if os.path.exists(evalDatasetFile):\n",
    "    print(f'Eval dataset {evalDatasetFile} exists')\n",
    "    ragEvalDataset = EmbeddingQAFinetuneDataset.from_json( evalDatasetFile)\n",
    "else:\n",
    "    print(f'Eval dataset {evalDatasetFile} does not exist: creating it.')\n",
    "    ragEvalDataset = myGenerateQuestionContextPairs(nodes[:10],\n",
    "                                                    llm=llmOai,\n",
    "                                                    num_questions_per_chunk=1)\n",
    "\n",
    "    # We can save the evaluation dataset as a json file for later use.\n",
    "    ragEvalDataset.save_json(evalDatasetFile)\n",
    "    print(f'Created {evalDatasetFile}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "383eda46-d8b0-4ac4-9690-d12957c4412f",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_evaluation(index, rag_eval_dataset, top_k_values, llm_judge, llm, n_queries_to_evaluate=20,num_work=1):\n",
    "    evaluation_results = {}\n",
    "\n",
    "    # ------------------- MRR and Hit Rate -------------------\n",
    "\n",
    "    for top_k in top_k_values:\n",
    "        # Get MRR and Hit Rate\n",
    "        retriever = index.as_retriever(similarity_top_k=top_k)\n",
    "        retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
    "            [\"mrr\", \"hit_rate\"], retriever=retriever\n",
    "        )\n",
    "        eval_results = await retriever_evaluator.aevaluate_dataset(rag_eval_dataset)\n",
    "        avg_mrr = sum(res.metric_vals_dict[\"mrr\"] for res in eval_results) / len(eval_results)\n",
    "        avg_hit_rate = sum(res.metric_vals_dict[\"hit_rate\"] for res in eval_results) / len(eval_results)\n",
    "\n",
    "        # Collect the evaluation results\n",
    "        evaluation_results[f\"mrr_@_{top_k}\"] = avg_mrr\n",
    "        evaluation_results[f\"hit_rate_@_{top_k}\"] = avg_hit_rate\n",
    "\n",
    "    # ------------------- Faithfulness and Relevancy -------------------\n",
    "\n",
    "    # Extract the questions from the dataset\n",
    "    queries = list(rag_eval_dataset.queries.values())\n",
    "    batch_eval_queries = queries[:n_queries_to_evaluate]\n",
    "\n",
    "    # Initiate the faithfulnes and relevancy evaluator objects\n",
    "    faithfulness_evaluator = FaithfulnessEvaluator(llm=llm_judge)\n",
    "    relevancy_evaluator = RelevancyEvaluator(llm=llm_judge)\n",
    "\n",
    "    # The batch evaluator runs the evaluation in batches\n",
    "    runner = BatchEvalRunner(\n",
    "        {\n",
    "            \"faithfulness\": faithfulness_evaluator,\n",
    "            \"relevancy\": relevancy_evaluator\n",
    "        },\n",
    "        workers=num_work,\n",
    "        show_progress=True,\n",
    "    )\n",
    "\n",
    "    # Get faithfulness and relevancy scores\n",
    "    query_engine = index.as_query_engine(llm=llm)\n",
    "    eval_results = await runner.aevaluate_queries(\n",
    "        query_engine, queries=batch_eval_queries\n",
    "    )\n",
    "    faithfulness_score = sum(result.passing for result in eval_results['faithfulness']) / len(eval_results['faithfulness'])\n",
    "    relevancy_score = sum(result.passing for result in eval_results['relevancy']) / len(eval_results['relevancy'])\n",
    "    evaluation_results[\"faithfulness\"] = faithfulness_score\n",
    "    evaluation_results[\"relevancy\"] = relevancy_score\n",
    "\n",
    "    return evaluation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "27fa5527-b9c8-4658-a44a-354345db57e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "numQueriesToEvaluate=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6d2f9d26-9ae1-45d2-992e-42dd199bd0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:58<00:00,  5.85s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:15<00:00,  1.32it/s]\n"
     ]
    }
   ],
   "source": [
    "# top_k_values = [2, 4, 6, 8, 10]\n",
    "top_k_values = [4]\n",
    "\n",
    "llmJudge = OpenAI(temperature=0, model=\"gpt-4o\")\n",
    "\n",
    "evaluation_results = await run_evaluation(index, \n",
    "                                          ragEvalDataset, \n",
    "                                          top_k_values, \n",
    "                                          llmJudge,                                          \n",
    "                                          llm=llm,\n",
    "                                          n_queries_to_evaluate=numQueriesToEvaluate,\n",
    "                                          num_work=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5c80e0b4-c29d-420a-86f2-3a300fc29fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'faithfulness': 1.0,\n",
      " 'hit_rate_@_4': 0.0,\n",
      " 'mrr_@_4': 0.0,\n",
      " 'relevancy': 0.9}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(evaluation_results, width=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c36a95-1fc8-48ca-98c6-6feb8d514d47",
   "metadata": {},
   "source": [
    "<pre>\n",
    "When k=2\n",
    "{'faithfulness': 0.9,\n",
    " 'hit_rate_@_2': 0.0,\n",
    " 'mrr_@_2': 0.0,\n",
    " 'relevancy': 0.9}\n",
    "</pre> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
