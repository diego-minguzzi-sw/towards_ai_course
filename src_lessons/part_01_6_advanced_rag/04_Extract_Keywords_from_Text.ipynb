{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d816c967-8c1e-42fc-bc46-d4c4acc8466c",
   "metadata": {},
   "source": [
    "# Example of how to extract keywords from a Text.\n",
    "Such keywords act as tags, help identifying the topic in later searches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c2255cbc-7fc3-4299-bf1f-6fa77a70b0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.llms.google_genai import GoogleGenAI\n",
    "from llama_index.embeddings.google_genai import GoogleGenAIEmbedding\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from typing import Any, List, Optional, Sequence, Union\n",
    "\n",
    "import asyncio\n",
    "import logging as log\n",
    "import os\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7311770a-7c55-4b62-a206-fa8316930bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_API_KEY_NAME='GOOGLE_API_KEY'\n",
    "assert GOOGLE_API_KEY_NAME in os.environ\n",
    "\n",
    "model_name= \"gemini-2.0-flash-lite\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6bfe06a8-3980-4b9b-b487-fba7521840b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "FORMAT_STRING = \"%(module)s.%(funcName)s():%(lineno)d %(asctime)s\\n[%(levelname)-5s] %(message)s\\n\"\n",
    "log.basicConfig(level= log.INFO, format=FORMAT_STRING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b20709db-db41-4832-b823-477e573099b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt( text: str):\n",
    "    return \"\"\"Given the following text, please extract the most relevant keywords or tags that will help identify the content of the text for later retrieval. \n",
    "    The keywords should be specific and concise, ideally representing the key themes, topics, concepts, people and products mentioned in the text.\n",
    "    Here is the text:\n",
    "    \n",
    "    %s\n",
    "    \n",
    "    Please provide a list of keywords or tags separated by commas. Do not output anything else than the keywords and the commas.\n",
    "    \"\"\" % text\n",
    "\n",
    "def get_keywords( text: str, model_name= \"gemini-2.0-flash-lite\"):\n",
    "    prompt= get_prompt( text)\n",
    "    log.info(f'prompt:\\n{prompt}\\n')\n",
    "\n",
    "    llm = GoogleGenAI(model=model_name)\n",
    "    resp = llm.complete(prompt)\n",
    "    log.info(f'resp: {resp}\\n')\n",
    "    str_resp = str(resp)\n",
    "    return [ raw_kw.strip() for raw_kw in str_resp.split(',') if len(raw_kw.strip())>0 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1f90656e-1cef-4c4a-8339-e6c97373bd6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "451742900.get_keywords():13 2025-04-06 16:31:33,069\n",
      "[INFO ] prompt:\n",
      "Given the following text, please extract the most relevant keywords or tags that will help identify the content of the text for later retrieval. \n",
      "    The keywords should be specific and concise, ideally representing the key themes, topics, concepts, people and products mentioned in the text.\n",
      "    Here is the text:\n",
      "    \n",
      "    \n",
      "Large Language Models (LLMs) have transformed natural language processing, but face significant challenges in widespread deployment due to their high runtime cost. \n",
      "In this paper, we introduce SeedLM, a novel post-training compression method that uses seeds of a pseudo-random generator to encode and compress model weights. \n",
      "Specifically, for each block of weights, we find a seed that is fed into a Linear Feedback Shift Register (LFSR) during inference to efficiently generate a random matrix. \n",
      "This matrix is then linearly combined with compressed coefficients to reconstruct the weight block. SeedLM reduces memory access and leverages idle compute cycles during inference, effectively speeding up memory-bound tasks by trading compute for fewer memory accesses. \n",
      "Unlike state-of-the-art methods that rely on calibration data, our approach is data-free and generalizes well across diverse tasks. Our experiments with Llama3 70B, which is particularly challenging, show zero-shot accuracy retention at 4- and 3-bit compression to be on par with or better than state-of-the-art methods, while maintaining performance comparable to FP16 baselines. \n",
      "Additionally, FPGA-based tests demonstrate that 4-bit SeedLM, as model size increases, approaches a 4x speed-up over an FP16 Llama 2/3 baseline.\n",
      "\n",
      "    \n",
      "    Please provide a list of keywords or tags separated by commas. Do not output anything else than the keywords and the commas.\n",
      "    \n",
      "\n",
      "\n",
      "_client._send_single_request():1025 2025-04-06 16:31:33,869\n",
      "[INFO ] HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-lite \"HTTP/1.1 200 OK\"\n",
      "\n",
      "models.generate_content():4934 2025-04-06 16:31:33,875\n",
      "[INFO ] AFC is enabled with max remote calls: 10.\n",
      "\n",
      "_client._send_single_request():1025 2025-04-06 16:31:34,658\n",
      "[INFO ] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-lite:generateContent \"HTTP/1.1 200 OK\"\n",
      "\n",
      "models.generate_content():4945 2025-04-06 16:31:34,662\n",
      "[INFO ] AFC remote call 1 is done.\n",
      "\n",
      "451742900.get_keywords():17 2025-04-06 16:31:34,664\n",
      "[INFO ] resp: Large Language Models, LLMs, SeedLM, post-training compression, model weights, Linear Feedback Shift Register, LFSR, inference, memory access, compute cycles, data-free, Llama3 70B, 4-bit compression, FPGA, speed-up, Llama 2/3\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large Language Models\n",
      "LLMs\n",
      "SeedLM\n",
      "post-training compression\n",
      "model weights\n",
      "Linear Feedback Shift Register\n",
      "LFSR\n",
      "inference\n",
      "memory access\n",
      "compute cycles\n",
      "data-free\n",
      "Llama3 70B\n",
      "4-bit compression\n",
      "FPGA\n",
      "speed-up\n",
      "Llama 2/3\n"
     ]
    }
   ],
   "source": [
    "text=\"\"\"\n",
    "Large Language Models (LLMs) have transformed natural language processing, but face significant challenges in widespread deployment due to their high runtime cost. \n",
    "In this paper, we introduce SeedLM, a novel post-training compression method that uses seeds of a pseudo-random generator to encode and compress model weights. \n",
    "Specifically, for each block of weights, we find a seed that is fed into a Linear Feedback Shift Register (LFSR) during inference to efficiently generate a random matrix. \n",
    "This matrix is then linearly combined with compressed coefficients to reconstruct the weight block. SeedLM reduces memory access and leverages idle compute cycles during inference, effectively speeding up memory-bound tasks by trading compute for fewer memory accesses. \n",
    "Unlike state-of-the-art methods that rely on calibration data, our approach is data-free and generalizes well across diverse tasks. Our experiments with Llama3 70B, which is particularly challenging, show zero-shot accuracy retention at 4- and 3-bit compression to be on par with or better than state-of-the-art methods, while maintaining performance comparable to FP16 baselines. \n",
    "Additionally, FPGA-based tests demonstrate that 4-bit SeedLM, as model size increases, approaches a 4x speed-up over an FP16 Llama 2/3 baseline.\n",
    "\"\"\"\n",
    "keywords= get_keywords(text)\n",
    "print( '\\n'.join( keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "018913a0-c49f-4aab-9235-ebce0e8d9811",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"\n",
    "We’re sharing the first models in the Llama 4 herd, which will enable people to build more personalized multimodal experiences.\n",
    "Llama 4 Scout, a 17 billion active parameter model with 16 experts, is the best multimodal model in the world in its class and is more powerful than all previous generation Llama models, while fitting in a single NVIDIA H100 GPU. Additionally, Llama 4 Scout offers an industry-leading context window of 10M and delivers better results than Gemma 3, Gemini 2.0 Flash-Lite, and Mistral 3.1 across a broad range of widely reported benchmarks.\n",
    "Llama 4 Maverick, a 17 billion active parameter model with 128 experts, is the best multimodal model in its class, beating GPT-4o and Gemini 2.0 Flash across a broad range of widely reported benchmarks, while achieving comparable results to the new DeepSeek v3 on reasoning and coding—at less than half the active parameters. Llama 4 Maverick offers a best-in-class performance to cost ratio with an experimental chat version scoring ELO of 1417 on LMArena.\n",
    "These models are our best yet thanks to distillation from Llama 4 Behemoth, a 288 billion active parameter model with 16 experts that is our most powerful yet and among the world’s smartest LLMs. Llama 4 Behemoth outperforms GPT-4.5, Claude Sonnet 3.7, and Gemini 2.0 Pro on several STEM benchmarks. Llama 4 Behemoth is still training, and we’re excited to share more details about it even while it’s still in flight.\n",
    "Download the Llama 4 Scout and Llama 4 Maverick models today on llama.com and Hugging Face. Try Meta AI built with Llama 4 in WhatsApp, Messenger, Instagram Direct, and on the web.\n",
    "As more people continue to use artificial intelligence to enhance their daily lives, it’s important that the leading models and systems are openly available so everyone can build the future of personalized experiences. Today, we’re excited to announce the most advanced suite of models that support the entire Llama ecosystem. We’re introducing Llama 4 Scout and Llama 4 Maverick, the first open-weight natively multimodal models with unprecedented context length support and our first built using a mixture-of-experts (MoE) architecture. We’re also previewing Llama 4 Behemoth, one of the smartest LLMs in the world and our most powerful yet to serve as a teacher for our new models.\n",
    "\n",
    "These Llama 4 models mark the beginning of a new era for the Llama ecosystem. We designed two efficient models in the Llama 4 series, Llama 4 Scout, a 17 billion active parameter model with 16 experts, and Llama 4 Maverick, a 17 billion active parameter model with 128 experts. The former fits on a single H100 GPU (with Int4 quantization) while the latter fits on a single H100 host. We also trained a teacher model, Llama 4 Behemoth, that outperforms GPT-4.5, Claude Sonnet 3.7, and Gemini 2.0 Pro on STEM-focused benchmarks such as MATH-500 and GPQA Diamond. While we’re not yet releasing Llama 4 Behemoth as it is still training, we’re excited to share more technical details about our approach.\n",
    "\n",
    "We continue to believe that openness drives innovation and is good for developers, good for Meta, and good for the world. We’re making Llama 4 Scout and Llama 4 Maverick available for download today on llama.com and Hugging Face so everyone can continue to build new experiences using our latest technology. We’ll also make them available via our partners in the coming days. You can also try Meta AI with Llama 4 starting today in WhatsApp, Messenger, Instagram Direct, and on the Meta.AI website.\n",
    "\n",
    "This is just the beginning for the Llama 4 collection. We believe that the most intelligent systems need to be capable of taking generalized actions, conversing naturally with humans, and working through challenging problems they haven’t seen before. Giving Llama superpowers in these areas will lead to better products for people on our platforms and more opportunities for developers to innovate on the next big consumer and business use cases. We’re continuing to research and prototype both models and products, and we’ll share more about our vision at LlamaCon on April 29—sign up to hear more.\n",
    "\n",
    "Whether you’re a developer building on top of our models, an enterprise integrating them into your workflows, or simply curious about the potential uses and benefits of AI, Llama 4 Scout and Llama 4 Maverick are the best choices for adding next-generation intelligence to your products. Today, we’re excited to share more about the four major parts of their development and insights into our research and design process. We also can’t wait to see the incredible new experiences the community builds with our new Llama 4 models.\n",
    "\n",
    "Pre-training\n",
    "These models represent the best of Llama, offering multimodal intelligence at a compelling price while outperforming models of significantly larger sizes. Building the next generation of Llama models required us to take several new approaches during pre-training.\n",
    "\n",
    "Our new Llama 4 models are our first models that use a mixture of experts (MoE) architecture. In MoE models, a single token activates only a fraction of the total parameters. MoE architectures are more compute efficient for training and inference and, given a fixed training FLOPs budget, delivers higher quality compared to a dense model.\n",
    "\n",
    "\n",
    "\n",
    "seamlessly integrate text and vision tokens into a unified model backbone. Early fusion is a major step forward, since it enables us to jointly pre-train the model with large amounts of unlabeled text, image, and video data. We also improved the vision encoder in Llama 4. This is based on MetaCLIP but trained separately in conjunction with a frozen Llama model to better adapt the encoder to the LLM.\n",
    "\n",
    "We developed a new training technique which we refer to as MetaP that allows us to reliably set critical model hyper-parameters such as per-layer learning rates and initialization scales. We found that chosen hyper-parameters transfer well across different values of batch size, model width, depth, and training tokens. Llama 4 enables open source fine-tuning efforts by pre-training on 200 languages, including over 100 with over 1 billion tokens each, and overall 10x more multilingual tokens than Llama 3.\n",
    "\n",
    "Additionally, we focus on efficient model training by using FP8 precision, without sacrificing quality and ensuring high model FLOPs utilization—while pre-training our Llama 4 Behemoth model using FP8 and 32K GPUs, we achieved 390 TFLOPs/GPU. The overall data mixture for training consisted of more than 30 trillion tokens, which is more than double the Llama 3 pre-training mixture and includes diverse text, image, and video datasets.\n",
    "\n",
    "We continued training the model in what we call “mid-training” to improve core capabilities with new training recipes including long context extension using specialized datasets. This enabled us to enhance model quality while also unlocking best-in-class 10M input context length for Llama 4 Scout.\n",
    "\n",
    "Post-training our new models\n",
    "Our newest models include smaller and larger options to accommodate a range of use cases and developer needs. Llama 4 Maverick offers unparalleled, industry-leading performance in image and text understanding, enabling the creation of sophisticated AI applications that bridge language barriers. As our product workhorse model for general assistant and chat use cases, Llama 4 Maverick is great for precise image understanding and creative writing.\n",
    "\n",
    "The biggest challenge while post-training the Llama 4 Maverick model was maintaining a balance between multiple input modalities, reasoning, and conversational abilities. For mixing modalities, we came up with a carefully curated curriculum strategy that does not trade-off performance compared to the individual modality expert models. With Llama 4, we revamped our post-training pipeline by adopting a different approach: lightweight supervised fine-tuning (SFT) > online reinforcement learning (RL) > lightweight direct preference optimization (DPO). A key learning was that SFT and DPO can over-constrain the model, restricting exploration during the online RL stage and leading to suboptimal accuracy, particularly in reasoning, coding, and math domains. To address this, we removed more than 50% of our data tagged as easy by using Llama models as a judge and did lightweight SFT on the remaining harder set. In the subsequent multimodal online RL stage, by carefully selecting harder prompts, we were able to achieve a step change in performance. Furthermore, we implemented a continuous online RL strategy, where we alternated between training the model and then using it to continually filter and retain only medium-to-hard difficulty prompts. This strategy proved highly beneficial in terms of compute and accuracy tradeoffs. We then did a lightweight DPO to handle corner cases related to model response quality, effectively achieving a good balance between the model’s intelligence and conversational abilities. Both the pipeline architecture and the continuous online RL strategy with adaptive data filtering culminated in an industry-leading, general-purpose chat model with state-of-the-art intelligence and image understanding capabilities.\n",
    "\n",
    "As a general purpose LLM, Llama 4 Maverick contains 17 billion active parameters, 128 experts, and 400 billion total parameters, offering high quality at a lower price compared to Llama 3.3 70B. Llama 4 Maverick is the best-in-class multimodal model, exceeding comparable models like GPT-4o and Gemini 2.0 on coding, reasoning, multilingual, long-context, and image benchmarks, and it’s competitive with the much larger DeepSeek v3.1 on coding and reasoning.\n",
    "\n",
    "\n",
    "Our smaller model, Llama 4 Scout, is a general purpose model with 17 billion active parameters, 16 experts, and 109 billion total parameters that delivers state-of-the-art performance for its class. Llama 4 Scout dramatically increases the supported context length from 128K in Llama 3 to an industry leading 10 million tokens. This opens up a world of possibilities, including multi-document summarization, parsing extensive user activity for personalized tasks, and reasoning over vast codebases.\n",
    "\n",
    "Llama 4 Scout is both pre-trained and post-trained with a 256K context length, which empowers the base model with advanced length generalization capability. We present compelling results in tasks such as retrieval with “retrieval needle in haystack” for text as well as cumulative negative log-likelihoods (NLLs) over 10 million tokens of code. A key innovation in the Llama 4 architecture is the use of interleaved attention layers without positional embeddings. Additionally, we employ inference time temperature scaling of attention to enhance length generalization. We call this the iRoPE architecture, where “i” stands for “interleaved” attention layers, highlighting the long-term goal of supporting “infinite” context length, and “RoPE” refers to the rotary position embeddings employed in most layers.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d51767c2-8f91-4d4b-ac9f-e273e6e6063a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "451742900.get_keywords():13 2025-04-06 16:31:34,699\n",
      "[INFO ] prompt:\n",
      "Given the following text, please extract the most relevant keywords or tags that will help identify the content of the text for later retrieval. \n",
      "    The keywords should be specific and concise, ideally representing the key themes, topics, concepts, people and products mentioned in the text.\n",
      "    Here is the text:\n",
      "    \n",
      "    \n",
      "We’re sharing the first models in the Llama 4 herd, which will enable people to build more personalized multimodal experiences.\n",
      "Llama 4 Scout, a 17 billion active parameter model with 16 experts, is the best multimodal model in the world in its class and is more powerful than all previous generation Llama models, while fitting in a single NVIDIA H100 GPU. Additionally, Llama 4 Scout offers an industry-leading context window of 10M and delivers better results than Gemma 3, Gemini 2.0 Flash-Lite, and Mistral 3.1 across a broad range of widely reported benchmarks.\n",
      "Llama 4 Maverick, a 17 billion active parameter model with 128 experts, is the best multimodal model in its class, beating GPT-4o and Gemini 2.0 Flash across a broad range of widely reported benchmarks, while achieving comparable results to the new DeepSeek v3 on reasoning and coding—at less than half the active parameters. Llama 4 Maverick offers a best-in-class performance to cost ratio with an experimental chat version scoring ELO of 1417 on LMArena.\n",
      "These models are our best yet thanks to distillation from Llama 4 Behemoth, a 288 billion active parameter model with 16 experts that is our most powerful yet and among the world’s smartest LLMs. Llama 4 Behemoth outperforms GPT-4.5, Claude Sonnet 3.7, and Gemini 2.0 Pro on several STEM benchmarks. Llama 4 Behemoth is still training, and we’re excited to share more details about it even while it’s still in flight.\n",
      "Download the Llama 4 Scout and Llama 4 Maverick models today on llama.com and Hugging Face. Try Meta AI built with Llama 4 in WhatsApp, Messenger, Instagram Direct, and on the web.\n",
      "As more people continue to use artificial intelligence to enhance their daily lives, it’s important that the leading models and systems are openly available so everyone can build the future of personalized experiences. Today, we’re excited to announce the most advanced suite of models that support the entire Llama ecosystem. We’re introducing Llama 4 Scout and Llama 4 Maverick, the first open-weight natively multimodal models with unprecedented context length support and our first built using a mixture-of-experts (MoE) architecture. We’re also previewing Llama 4 Behemoth, one of the smartest LLMs in the world and our most powerful yet to serve as a teacher for our new models.\n",
      "\n",
      "These Llama 4 models mark the beginning of a new era for the Llama ecosystem. We designed two efficient models in the Llama 4 series, Llama 4 Scout, a 17 billion active parameter model with 16 experts, and Llama 4 Maverick, a 17 billion active parameter model with 128 experts. The former fits on a single H100 GPU (with Int4 quantization) while the latter fits on a single H100 host. We also trained a teacher model, Llama 4 Behemoth, that outperforms GPT-4.5, Claude Sonnet 3.7, and Gemini 2.0 Pro on STEM-focused benchmarks such as MATH-500 and GPQA Diamond. While we’re not yet releasing Llama 4 Behemoth as it is still training, we’re excited to share more technical details about our approach.\n",
      "\n",
      "We continue to believe that openness drives innovation and is good for developers, good for Meta, and good for the world. We’re making Llama 4 Scout and Llama 4 Maverick available for download today on llama.com and Hugging Face so everyone can continue to build new experiences using our latest technology. We’ll also make them available via our partners in the coming days. You can also try Meta AI with Llama 4 starting today in WhatsApp, Messenger, Instagram Direct, and on the Meta.AI website.\n",
      "\n",
      "This is just the beginning for the Llama 4 collection. We believe that the most intelligent systems need to be capable of taking generalized actions, conversing naturally with humans, and working through challenging problems they haven’t seen before. Giving Llama superpowers in these areas will lead to better products for people on our platforms and more opportunities for developers to innovate on the next big consumer and business use cases. We’re continuing to research and prototype both models and products, and we’ll share more about our vision at LlamaCon on April 29—sign up to hear more.\n",
      "\n",
      "Whether you’re a developer building on top of our models, an enterprise integrating them into your workflows, or simply curious about the potential uses and benefits of AI, Llama 4 Scout and Llama 4 Maverick are the best choices for adding next-generation intelligence to your products. Today, we’re excited to share more about the four major parts of their development and insights into our research and design process. We also can’t wait to see the incredible new experiences the community builds with our new Llama 4 models.\n",
      "\n",
      "Pre-training\n",
      "These models represent the best of Llama, offering multimodal intelligence at a compelling price while outperforming models of significantly larger sizes. Building the next generation of Llama models required us to take several new approaches during pre-training.\n",
      "\n",
      "Our new Llama 4 models are our first models that use a mixture of experts (MoE) architecture. In MoE models, a single token activates only a fraction of the total parameters. MoE architectures are more compute efficient for training and inference and, given a fixed training FLOPs budget, delivers higher quality compared to a dense model.\n",
      "\n",
      "\n",
      "\n",
      "seamlessly integrate text and vision tokens into a unified model backbone. Early fusion is a major step forward, since it enables us to jointly pre-train the model with large amounts of unlabeled text, image, and video data. We also improved the vision encoder in Llama 4. This is based on MetaCLIP but trained separately in conjunction with a frozen Llama model to better adapt the encoder to the LLM.\n",
      "\n",
      "We developed a new training technique which we refer to as MetaP that allows us to reliably set critical model hyper-parameters such as per-layer learning rates and initialization scales. We found that chosen hyper-parameters transfer well across different values of batch size, model width, depth, and training tokens. Llama 4 enables open source fine-tuning efforts by pre-training on 200 languages, including over 100 with over 1 billion tokens each, and overall 10x more multilingual tokens than Llama 3.\n",
      "\n",
      "Additionally, we focus on efficient model training by using FP8 precision, without sacrificing quality and ensuring high model FLOPs utilization—while pre-training our Llama 4 Behemoth model using FP8 and 32K GPUs, we achieved 390 TFLOPs/GPU. The overall data mixture for training consisted of more than 30 trillion tokens, which is more than double the Llama 3 pre-training mixture and includes diverse text, image, and video datasets.\n",
      "\n",
      "We continued training the model in what we call “mid-training” to improve core capabilities with new training recipes including long context extension using specialized datasets. This enabled us to enhance model quality while also unlocking best-in-class 10M input context length for Llama 4 Scout.\n",
      "\n",
      "Post-training our new models\n",
      "Our newest models include smaller and larger options to accommodate a range of use cases and developer needs. Llama 4 Maverick offers unparalleled, industry-leading performance in image and text understanding, enabling the creation of sophisticated AI applications that bridge language barriers. As our product workhorse model for general assistant and chat use cases, Llama 4 Maverick is great for precise image understanding and creative writing.\n",
      "\n",
      "The biggest challenge while post-training the Llama 4 Maverick model was maintaining a balance between multiple input modalities, reasoning, and conversational abilities. For mixing modalities, we came up with a carefully curated curriculum strategy that does not trade-off performance compared to the individual modality expert models. With Llama 4, we revamped our post-training pipeline by adopting a different approach: lightweight supervised fine-tuning (SFT) > online reinforcement learning (RL) > lightweight direct preference optimization (DPO). A key learning was that SFT and DPO can over-constrain the model, restricting exploration during the online RL stage and leading to suboptimal accuracy, particularly in reasoning, coding, and math domains. To address this, we removed more than 50% of our data tagged as easy by using Llama models as a judge and did lightweight SFT on the remaining harder set. In the subsequent multimodal online RL stage, by carefully selecting harder prompts, we were able to achieve a step change in performance. Furthermore, we implemented a continuous online RL strategy, where we alternated between training the model and then using it to continually filter and retain only medium-to-hard difficulty prompts. This strategy proved highly beneficial in terms of compute and accuracy tradeoffs. We then did a lightweight DPO to handle corner cases related to model response quality, effectively achieving a good balance between the model’s intelligence and conversational abilities. Both the pipeline architecture and the continuous online RL strategy with adaptive data filtering culminated in an industry-leading, general-purpose chat model with state-of-the-art intelligence and image understanding capabilities.\n",
      "\n",
      "As a general purpose LLM, Llama 4 Maverick contains 17 billion active parameters, 128 experts, and 400 billion total parameters, offering high quality at a lower price compared to Llama 3.3 70B. Llama 4 Maverick is the best-in-class multimodal model, exceeding comparable models like GPT-4o and Gemini 2.0 on coding, reasoning, multilingual, long-context, and image benchmarks, and it’s competitive with the much larger DeepSeek v3.1 on coding and reasoning.\n",
      "\n",
      "\n",
      "Our smaller model, Llama 4 Scout, is a general purpose model with 17 billion active parameters, 16 experts, and 109 billion total parameters that delivers state-of-the-art performance for its class. Llama 4 Scout dramatically increases the supported context length from 128K in Llama 3 to an industry leading 10 million tokens. This opens up a world of possibilities, including multi-document summarization, parsing extensive user activity for personalized tasks, and reasoning over vast codebases.\n",
      "\n",
      "Llama 4 Scout is both pre-trained and post-trained with a 256K context length, which empowers the base model with advanced length generalization capability. We present compelling results in tasks such as retrieval with “retrieval needle in haystack” for text as well as cumulative negative log-likelihoods (NLLs) over 10 million tokens of code. A key innovation in the Llama 4 architecture is the use of interleaved attention layers without positional embeddings. Additionally, we employ inference time temperature scaling of attention to enhance length generalization. We call this the iRoPE architecture, where “i” stands for “interleaved” attention layers, highlighting the long-term goal of supporting “infinite” context length, and “RoPE” refers to the rotary position embeddings employed in most layers.\n",
      "\n",
      "\n",
      "    \n",
      "    Please provide a list of keywords or tags separated by commas. Do not output anything else than the keywords and the commas.\n",
      "    \n",
      "\n",
      "\n",
      "_client._send_single_request():1025 2025-04-06 16:31:37,725\n",
      "[INFO ] HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-lite \"HTTP/1.1 200 OK\"\n",
      "\n",
      "models.generate_content():4934 2025-04-06 16:31:37,731\n",
      "[INFO ] AFC is enabled with max remote calls: 10.\n",
      "\n",
      "_client._send_single_request():1025 2025-04-06 16:31:40,202\n",
      "[INFO ] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-lite:generateContent \"HTTP/1.1 200 OK\"\n",
      "\n",
      "models.generate_content():4945 2025-04-06 16:31:40,211\n",
      "[INFO ] AFC remote call 1 is done.\n",
      "\n",
      "451742900.get_keywords():17 2025-04-06 16:31:40,212\n",
      "[INFO ] resp: Llama 4, Llama 4 Scout, Llama 4 Maverick, Llama 4 Behemoth, multimodal models, open-weight, mixture-of-experts (MoE), context length, NVIDIA H100 GPU, GPT-4o, Gemini 2.0, DeepSeek v3, Meta AI, WhatsApp, Messenger, Instagram Direct, Meta.AI, pre-training, post-training, FP8, iRoPE architecture, long context, benchmarks, AI, LLM, Meta, Hugging Face, llama.com\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama 4\n",
      "Llama 4 Scout\n",
      "Llama 4 Maverick\n",
      "Llama 4 Behemoth\n",
      "multimodal models\n",
      "open-weight\n",
      "mixture-of-experts (MoE)\n",
      "context length\n",
      "NVIDIA H100 GPU\n",
      "GPT-4o\n",
      "Gemini 2.0\n",
      "DeepSeek v3\n",
      "Meta AI\n",
      "WhatsApp\n",
      "Messenger\n",
      "Instagram Direct\n",
      "Meta.AI\n",
      "pre-training\n",
      "post-training\n",
      "FP8\n",
      "iRoPE architecture\n",
      "long context\n",
      "benchmarks\n",
      "AI\n",
      "LLM\n",
      "Meta\n",
      "Hugging Face\n",
      "llama.com\n"
     ]
    }
   ],
   "source": [
    "keywords= get_keywords(text)\n",
    "print( '\\n'.join( keywords))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
