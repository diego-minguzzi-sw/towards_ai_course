{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88180e20-622e-473b-bab2-0589bb53c431",
   "metadata": {},
   "source": [
    "# LLM as a Judge\n",
    "\n",
    "An LLM is uses as a judge to rank (to assign a score) to determine how much a chunk is relevant to a given query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a68fb5a-1a93-4fa1-a893-470beb575db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import json\n",
    "import os\n",
    "\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69e72488-b4a2-41d5-ae95-39561b754694",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPEN_AI_KEY_NAME='OPENAI_API_KEY'\n",
    "assert OPEN_AI_KEY_NAME in os.environ\n",
    "\n",
    "TAI_DATASET_ROOT_ENV_VAR='TAI_DATASET_ROOT'\n",
    "assert TAI_DATASET_ROOT_ENV_VAR in os.environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7925f9a-dba0-4890-aada-85cae1c712b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "⚠️ It looks like you upgraded from a version below 0.5.6 and could benefit from vacuuming your database. Run chromadb utils vacuum --help for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorDbPath: /home/minguzzi/repo/towards_ai_course/dataset/ai_tutor_knowledge_vectdb\n",
      "top results:\n",
      "\t RAG (Retrieval-Augmented Generation) works by combining retrieval-based methods with generative models to enhance the quality of generated text. It involves retrieving relevant information from a large dataset or knowledge source using retriever models and then incorporating this retrieved information into the generative model's text generation process. This approach aims to improve the coherence, relevance, and factual accuracy of the generated text by leveraging external knowledge during the generation process.\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n"
     ]
    }
   ],
   "source": [
    "assert TAI_DATASET_ROOT_ENV_VAR in os.environ\n",
    "vectorDbPath = os.path.join( os.environ[TAI_DATASET_ROOT_ENV_VAR], 'ai_tutor_knowledge_vectdb')\n",
    "print(f'vectorDbPath: {vectorDbPath}')\n",
    "\n",
    "# Load the vector store from the local storage.\n",
    "db = chromadb.PersistentClient(path=vectorDbPath)\n",
    "chroma_collection = db.get_collection(\"ai_tutor_knowledge\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "# Create the index based on the vector store.\n",
    "index = VectorStoreIndex.from_vector_store(vector_store=vector_store)\n",
    "\n",
    "query_engine = index.as_query_engine(similarity_top_k=4)\n",
    "\n",
    "res = query_engine.query(\"Explain how RAG works?\")\n",
    "\n",
    "print(f\"top results:\")\n",
    "print(\"\\t\", res.response)\n",
    "print(\"-_\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c32e8e2-b6d8-4e6c-9328-74e06a1e47c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d00a94a-ffc8-47ff-ba78-d87dc1041c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class OrderedNodes(BaseModel):\n",
    "  \"\"\"A list of nodes with the ids and assigned scores.\"\"\"\n",
    "  node_id: list\n",
    "  score: list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dedd1f2f-4944-43fb-a24c-5b5c97ea6b12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(metadata={'prompt_type': <PromptType.CUSTOM: 'custom'>}, template_vars=['nodes_list', 'user_query'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, template=\"\\n    You receive a qurey along with a list of nodes' text and their ids. Your task is to assign score\\n    to each node based on its contextually closeness to the given query. The final output is each\\n    node id along with its proximity score.\\n    Here is the list of nodes:\\n    {nodes_list}\\n\\n    And the following is the query:\\n    {user_query}\\n\\n    Score each of the nodes based on their text and their relevancy to the provided query.\\n    The score must be a decimal number between 0 an 1 so we can rank them.\")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core.prompts import PromptTemplate\n",
    "\n",
    "prompt_tmpl = PromptTemplate(\n",
    "    \"\"\"\n",
    "    You receive a qurey along with a list of nodes' text and their ids. Your task is to assign score\n",
    "    to each node based on its contextually closeness to the given query. The final output is each\n",
    "    node id along with its proximity score.\n",
    "    Here is the list of nodes:\n",
    "    {nodes_list}\n",
    "\n",
    "    And the following is the query:\n",
    "    {user_query}\n",
    "\n",
    "    Score each of the nodes based on their text and their relevancy to the provided query.\n",
    "    The score must be a decimal number between 0 an 1 so we can rank them.\"\"\"\n",
    "  )\n",
    "prompt_tmpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92574d5f-4568-4536-a0a4-dce16980a167",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "def judger(nodes, query):\n",
    "\n",
    "  # The model's output template\n",
    "  class OrderedNodes(BaseModel):\n",
    "    \"\"\"A node with the id and assigned score.\"\"\"\n",
    "    node_id: list\n",
    "    score: list\n",
    "\n",
    "  # Prepare the nodes and wrap them in <NODE></NODE> identifier, as well as the query\n",
    "  the_nodes=\"\"\n",
    "  for idx, item in enumerate(nodes):\n",
    "    the_nodes += f\"<NODE{idx+1}>\\nNode ID: {item.node_id}\\nText: {item.text}\\n</NODE{idx+1}>\\n\"\n",
    "\n",
    "  query = \"<QUERY>\\n{}\\n</QUERY>\".format(query)\n",
    "\n",
    "  # Define the prompt template\n",
    "  prompt_tmpl = PromptTemplate(\n",
    "    \"\"\"\n",
    "    You receive a qurey along with a list of nodes' text and their ids. Your task is to assign score\n",
    "    to each node based on its contextually closeness to the given query. The final output is each\n",
    "    node id along with its proximity score.\n",
    "    Here is the list of nodes:\n",
    "    {nodes_list}\n",
    "\n",
    "    And the following is the query:\n",
    "    {user_query}\n",
    "\n",
    "    Score each of the nodes based on their text and their relevancy to the provided query.\n",
    "    The score must be a decimal number between 0 an 1 so we can rank them.\"\"\"\n",
    "  )\n",
    "\n",
    "  # Define the an instance of GPT-4o-mini and send the request\n",
    "  llm = OpenAI(model=\"gpt-4o-mini\")\n",
    "  ordered_nodes = llm.structured_predict(\n",
    "    OrderedNodes, prompt_tmpl, nodes_list=the_nodes, user_query=query\n",
    "  )\n",
    "\n",
    "  return ordered_nodes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
