{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c8d8762-a77f-4bfb-a5e3-be9614f7cf9b",
   "metadata": {},
   "source": [
    "Metadata Filtering\n",
    "Metadata filtering:\n",
    "\n",
    "- extracting keywords from every chunk\n",
    "- at seatch time, filter appropriate chunks based on the metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f66e2e0-b583-4f78-95aa-c57e9ada0bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.llms.google_genai import GoogleGenAI\n",
    "from llama_index.embeddings.google_genai import GoogleGenAIEmbedding\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from typing import Any, List, Optional, Sequence, Union\n",
    "\n",
    "import asyncio\n",
    "import google.genai.errors\n",
    "import json\n",
    "import logging as log\n",
    "import os\n",
    "import pprint\n",
    "import qdrant_client\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c0dd277-5051-456a-af6c-99d77e8bfeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57980cda-895d-41af-87c3-fb6819e6cd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "FORMAT_STRING = \"%(module)s.%(funcName)s():%(lineno)d %(asctime)s\\n[%(levelname)-5s] %(message)s\\n\"\n",
    "log.basicConfig(level= log.ERROR, format=FORMAT_STRING)\n",
    "log.info('Info text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e66f90da-a68d-47da-9f01-677aae22128e",
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_API_KEY_NAME='GOOGLE_API_KEY'\n",
    "assert GOOGLE_API_KEY_NAME in os.environ\n",
    "\n",
    "OPEN_AI_KEY_NAME='OPENAI_API_KEY'\n",
    "assert OPEN_AI_KEY_NAME in os.environ\n",
    "\n",
    "TAI_DATASET_ROOT_NAME='TAI_DATASET_ROOT'\n",
    "assert TAI_DATASET_ROOT_NAME in os.environ\n",
    "\n",
    "model_name= \"gemini-2.0-flash-lite\"\n",
    "num_data_samples= 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "198b9780-e2d1-48e1-8d05-0a810c50c457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paul Graham is a prominent figure in the tech world, known for his work as a computer programmer, essayist, investor, and co-founder of the influential startup accelerator, **Y Combinator**.\n",
      "\n",
      "Here's a breakdown of his key contributions:\n",
      "\n",
      "*   **Computer Programmer:** He's a skilled programmer, particularly in Lisp. He wrote the first web application, Viaweb, which was later acquired by Yahoo! and became Yahoo! Store.\n",
      "*   **Essayist:** Graham is well-known for his insightful and thought-provoking essays on a wide range of topics, including:\n",
      "    *   **Startups and Entrepreneurship:** His essays are highly regarded in the startup community, offering advice on building companies, raising funding, and avoiding common pitfalls.\n",
      "    *   **Programming and Technology:** He writes about programming languages, software development, and the evolution of technology.\n",
      "    *   **Philosophy and Culture:** He explores broader themes related to society, education, and the pursuit of knowledge.\n",
      "    *   **His essays are available on his website, paulgraham.com.**\n",
      "*   **Investor:** He's a successful investor, primarily through Y Combinator. He's invested in numerous successful startups, including Airbnb, Dropbox, Reddit, and Stripe.\n",
      "*   **Co-founder of Y Combinator:** Y Combinator is one of the most successful startup accelerators in the world. It provides funding, mentorship, and networking opportunities to early-stage startups. Graham's role was crucial in shaping Y Combinator's philosophy and approach.\n",
      "*   **Author:** He has written several books, including \"On Lisp\" (a classic book on Lisp programming) and \"Hackers & Painters: Big Ideas from the Computer Age\" (a collection of his essays).\n",
      "\n",
      "In summary, Paul Graham is a highly influential figure in the tech and startup world, known for his programming skills, insightful writing, investment acumen, and role in building Y Combinator. His work has had a significant impact on the development of the internet and the startup ecosystem.\n"
     ]
    }
   ],
   "source": [
    "llm = GoogleGenAI(model=model_name)\n",
    "resp = llm.complete(\"Who is Paul Graham?\")\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00123944-ef83-4eba-9ce3-0e0dda31a0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pprint.pp(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf773706-eaa7-44d2-bce0-ace9351b5987",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llms.function_calling import FunctionCallingLLM\n",
    "from llama_index.core.base.llms.types import ChatMessage\n",
    "from llama_index.core.constants import DEFAULT_TEMPERATURE, DEFAULT_NUM_OUTPUTS\n",
    "\n",
    "class MyLLM(FunctionCallingLLM):\n",
    "    def __init__(self, \n",
    "                 model_name: str,\n",
    "                 temperature: float = DEFAULT_TEMPERATURE,\n",
    "                 max_tokens: Optional[int] = None,):\n",
    "        # Initialize the base class and the GoogleGenAI client\n",
    "        super().__init__(model= model_name, temperature=temperature, max_tokens=max_tokens)\n",
    "        self._llm = GoogleGenAI(model=model_name)\n",
    "        self._num_achat_requests = 0\n",
    "\n",
    "    @property\n",
    "    def metadata(self): return self._llm.metadata\n",
    "\n",
    "    def complete( self, prompt: str, formatted: bool = False, **kwargs: Any):\n",
    "        log.info('complete executed.')\n",
    "        try:\n",
    "            return self._llm.complete( prompt, formatted, **kwargs)\n",
    "        except Exception as exc:\n",
    "            log.error(f'complete: exception:{type(my_llm).__name__}: {exc.message}')\n",
    "            raise\n",
    "\n",
    "    async def acomplete(self, prompt: str, formatted: bool = False, **kwargs: Any):        \n",
    "        log.info('acomplete executed.')\n",
    "        result = await self._llm.acomplete( prompt, formatted, **kwargs)\n",
    "        return result\n",
    "\n",
    "    def stream_complete( self, prompt: str, formatted: bool = False, **kwargs: Any):\n",
    "        log.info('stream_complete executed.')\n",
    "        return self._llm.stream_complete( prompt, formatted, **kwargs)\n",
    "\n",
    "    async def astream_complete( self, prompt: str, formatted: bool = False, **kwargs: Any):\n",
    "        log.info('astream_complete executed.')\n",
    "        result = await self._llm.astream_complete( prompt, formatted, **kwargs)    \n",
    "        return result\n",
    "\n",
    "    def chat(self, messages: Sequence[ChatMessage], **kwargs: Any):\n",
    "        log.info('chat executed.')\n",
    "        return self._llm.chat( messages, **kwargs)    \n",
    "\n",
    "    async def achat(self, messages: Sequence[ChatMessage], **kwargs: Any):\n",
    "        \n",
    "        try:\n",
    "            result = await self._llm.achat( messages, **kwargs)    \n",
    "            self._num_achat_requests += 1\n",
    "            return result\n",
    "        except google.genai.errors.ClientError as exc:\n",
    "            log.error(f'achat: ClientError:{type(exc).__name__}: {exc.message}')\n",
    "            if (exc.code==429):\n",
    "                log.error(f'achat: RESOURCE EXAUSTED....................................................... num reqs:{self._num_achat_requests}')    \n",
    "            else:\n",
    "                log.error('achat: NOT RESOURCE EXAUSTED...................................................')    \n",
    "                raise\n",
    "        except Exception as exc:\n",
    "            log.error(f'achat: exception:{type(exc).__name__}: {exc.message}')\n",
    "            raise\n",
    "\n",
    "        await asyncio.sleep( 60)\n",
    "        self._num_achat_requests = 0\n",
    "        log.error('Second attempt...........................................................................')    \n",
    "        try:\n",
    "            result = await self._llm.achat( messages, **kwargs)    \n",
    "            self._num_achat_requests += 1\n",
    "            log.error('Second attempt executed ..................................................................')    \n",
    "            return result\n",
    "        except Exception as exc:\n",
    "            log.error(f'achat: exception:{type(exc).__name__}: {exc.message}')\n",
    "            raise\n",
    "        \n",
    "    def stream_chat(self, messages: Sequence[ChatMessage], **kwargs: Any):\n",
    "        log.info('stream_chat executed.')\n",
    "        return self._llm.stream_chat( messages, **kwargs)    \n",
    "\n",
    "    async def astream_chat(self, messages: Sequence[ChatMessage], **kwargs: Any):\n",
    "        log.info('astream_chat executed.')\n",
    "        response= await self._llm.astream_chat( messages, **kwargs)            \n",
    "        return response\n",
    "\n",
    "    def _prepare_chat_with_tools(\n",
    "        self,\n",
    "        tools: Sequence[\"BaseTool\"],\n",
    "        user_msg: Optional[Union[str, ChatMessage]] = None,\n",
    "        chat_history: Optional[List[ChatMessage]] = None,\n",
    "        verbose: bool = False,\n",
    "        allow_parallel_tool_calls: bool = False,\n",
    "        tool_choice: Union[str, dict] = \"auto\",\n",
    "        strict: Optional[bool] = None,\n",
    "        **kwargs: Any):\n",
    "        return self._llm._prepare_chat_with_tools(tools, \n",
    "                                                  user_msg, \n",
    "                                                  chat_history, \n",
    "                                                  verbose, \n",
    "                                                  allow_parallel_tools_calls, \n",
    "                                                  tool_choice,\n",
    "                                                  strict,\n",
    "                                                  **kwargs)        \n",
    "\n",
    "my_llm = MyLLM(model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10a05a84-7c86-4274-b9ab-a759268a03e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_llm:MyLLM\n"
     ]
    }
   ],
   "source": [
    "print(f'my_llm:{type(my_llm).__name__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28b50670-84c4-4727-9154-19ee30d61d76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nresp = my_llm.complete(\"Who is Paul Graham?\")\\nprint(resp)\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "resp = my_llm.complete(\"Who is Paul Graham?\")\n",
    "print(resp)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10eec6e3-9a69-4296-83b0-3121262fced0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.llm = my_llm\n",
    "Settings.embed_model = GoogleGenAIEmbedding(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c43ede-a8a9-4223-a49e-9dd5dd3138b9",
   "metadata": {},
   "source": [
    "## Create the Qdrant Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c603bbe4-740e-4549-9f28-3f352d5e394c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/minguzzi/repo/towards_ai_course/dataset/qdrant_vect_db_ai_tutor'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_db_path= os.path.join( os.environ[TAI_DATASET_ROOT_NAME],'qdrant_vect_db_ai_tutor')\n",
    "vector_db_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d29938a-02ab-44bf-8959-d45e8d5acef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/minguzzi/repo/towards_ai_course/dataset/ai_tutor_knowledge/ai_tutor_knowledge.jsonl'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_tutor_knowledge_file_path= os.path.join( os.environ[TAI_DATASET_ROOT_NAME],'ai_tutor_knowledge','ai_tutor_knowledge.jsonl')\n",
    "ai_tutor_knowledge_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3da6cf5-88c9-4a61-a4da-9237bdb4e8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant_client = QdrantClient(path=vector_db_path)\n",
    "\n",
    "vector_store = QdrantVectorStore( client=qdrant_client,  collection_name=\"ai_tutor_knowledge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cae00a7a-6d08-4c37-b661-4fdd0b9fc8c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(ai_tutor_knowledge_file_path, \"r\") as file:\n",
    "    ai_tutor_knowledge = [json.loads(line) for line in file][0:num_data_samples]\n",
    "    \n",
    "ai_tutor_knowledge[1]['content']\n",
    "len(ai_tutor_knowledge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea46befc-43ad-42e9-9758-6a8f62e6ad1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = ai_tutor_knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8475e2c4-5bd0-4f94-aef1-e0e70cf8e3b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['title', 'tokens', 'source']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "from llama_index.core import Document\n",
    "\n",
    "def create_docs_from_list(data_list: List[dict]) -> List[Document]:\n",
    "    documents = []\n",
    "    for data in data_list:\n",
    "        documents.append(\n",
    "            Document(\n",
    "                doc_id=data[\"doc_id\"],\n",
    "                text=data[\"content\"],\n",
    "                metadata={  # type: ignore\n",
    "                    \"url\": data[\"url\"],\n",
    "                    \"title\": data[\"name\"],\n",
    "                    \"tokens\": data[\"tokens\"],\n",
    "                    \"source\": data[\"source\"],\n",
    "                },\n",
    "                excluded_llm_metadata_keys=[\n",
    "                    \"title\",\n",
    "                    \"tokens\",\n",
    "                    \"source\",\n",
    "                ],\n",
    "                excluded_embed_metadata_keys=[\n",
    "                    \"url\",\n",
    "                    \"tokens\",\n",
    "                    \"source\",\n",
    "                ],\n",
    "            )\n",
    "        )\n",
    "    return documents\n",
    "\n",
    "doc = create_docs_from_list(documents)\n",
    "doc[1].excluded_llm_metadata_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78521c1-57db-4d8d-8dd1-03770c77021f",
   "metadata": {},
   "source": [
    "<b> Split the text into Chunks</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a28b4f9-b792-4584-84dc-e64f767a40fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import TokenTextSplitter\n",
    "\n",
    "# Define the splitter object that split the text into segments with 512 tokens,\n",
    "# with a 128 overlap between the segments.\n",
    "text_splitter = TokenTextSplitter(separator=\" \", chunk_size=512, chunk_overlap=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c44464-e07e-4d5f-94db-eb18fca76983",
   "metadata": {},
   "source": [
    "<B>Uses a predefined pipeline to generate the tags for each chunk</B>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d2b73081-7312-417c-aa66-aabe1351738f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7284251aff9040a481cc49d01486f118",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3375812736.achat():51 2025-04-05 18:35:30,498█                                                                                                               | 17/63 [00:03<00:08,  5.68it/s]\n",
      "[ERROR] achat: ClientError:ClientError: Resource has been exhausted (e.g. check quota).\n",
      "\n",
      "3375812736.achat():53 2025-04-05 18:35:30,500\n",
      "[ERROR] achat: RESOURCE EXAUSTED....................................................... num reqs:17\n",
      "\n",
      "3375812736.achat():51 2025-04-05 18:35:30,596\n",
      "[ERROR] achat: ClientError:ClientError: Resource has been exhausted (e.g. check quota).\n",
      "\n",
      "3375812736.achat():53 2025-04-05 18:35:30,598\n",
      "[ERROR] achat: RESOURCE EXAUSTED....................................................... num reqs:18\n",
      "\n",
      "3375812736.achat():51 2025-04-05 18:35:31,044█████▊                                                                                                          | 19/63 [00:04<00:08,  5.23it/s]\n",
      "[ERROR] achat: ClientError:ClientError: Resource has been exhausted (e.g. check quota).\n",
      "\n",
      "3375812736.achat():53 2025-04-05 18:35:31,046\n",
      "[ERROR] achat: RESOURCE EXAUSTED....................................................... num reqs:19\n",
      "\n",
      "3375812736.achat():51 2025-04-05 18:36:04,681████████████████████████████████████████████████████████████████████▌                                           | 45/63 [00:37<00:18,  1.04s/it]\n",
      "[ERROR] achat: ClientError:ClientError: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "\n",
      "3375812736.achat():53 2025-04-05 18:36:04,683\n",
      "[ERROR] achat: RESOURCE EXAUSTED....................................................... num reqs:45\n",
      "\n",
      "3375812736.achat():63 2025-04-05 18:36:30,529\n",
      "[ERROR] Second attempt...........................................................................\n",
      "\n",
      "3375812736.achat():63 2025-04-05 18:36:30,601\n",
      "[ERROR] Second attempt...........................................................................\n",
      "\n",
      "3375812736.achat():63 2025-04-05 18:36:31,049\n",
      "[ERROR] Second attempt...........................................................................\n",
      "\n",
      "3375812736.achat():67 2025-04-05 18:36:31,413\n",
      "[ERROR] Second attempt executed ..................................................................\n",
      "\n",
      "3375812736.achat():67 2025-04-05 18:36:31,563██████████████████████████████████████████████████████████████████████▉                                         | 46/63 [01:04<02:30,  8.87s/it]\n",
      "[ERROR] Second attempt executed ..................................................................\n",
      "\n",
      "3375812736.achat():67 2025-04-05 18:36:32,243███████████████████████████████████████████████████████████████████████████▊                                    | 48/63 [01:05<01:08,  4.58s/it]\n",
      "[ERROR] Second attempt executed ..................................................................\n",
      "\n",
      "3375812736.achat():63 2025-04-05 18:37:04,708████████████████████████████████████████████████████████████████████████████████████████████████████████▊       | 60/63 [01:08<00:01,  1.54it/s]\n",
      "[ERROR] Second attempt...........................................................................\n",
      "\n",
      "3375812736.achat():67 2025-04-05 18:37:05,719\n",
      "[ERROR] Second attempt executed ..................................................................\n",
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [01:39<00:00,  1.57s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a117d2b0f744e8a9fe4bb87aa3295c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minguzzi/repo/towards_ai_course/venv/lib/python3.10/site-packages/llama_index/vector_stores/qdrant/base.py:703: UserWarning: Payload indexes have no effect in the local Qdrant. Please use server Qdrant if you need payload indexes.\n",
      "  self._client.create_payload_index(\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.extractors import (\n",
    "    KeywordExtractor,\n",
    ")\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "\n",
    "# Create the pipeline to apply the transformation on each chunk,\n",
    "# and store the transformed text in the chroma vector store.\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        text_splitter,\n",
    "        KeywordExtractor(keywords=10, llm=my_llm),\n",
    "        GoogleGenAIEmbedding(model=\"text-embedding-3-small\"),\n",
    "    ],\n",
    "    vector_store=vector_store,\n",
    ")\n",
    "\n",
    "# Run the transformation pipeline.\n",
    "nodes = pipeline.run(documents=doc, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7fe51ddf-8fce-4f0d-8ea7-f1aa680f9ce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ed3421ed-ad2b-41b5-b5d3-203684518c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT, Hugging Face, Kubernetes, Docker, Model Deployment, FastAPI, Uvicorn, Minikube, Scalability, Containerization\n",
      "BERT, HuggingFace, Kubernetes, Model Deployment, Docker, Minikube, Kubectl, GitHub, Containerization, MLops\n",
      "NER, Ecommerce, Machine Learning, Custom Model, Github, Data Preparation, Spacy, Mobile Phone, Product Description, Annotations\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(nodes[0].metadata['excerpt_keywords'])\n",
    "print(nodes[1].metadata['excerpt_keywords'])\n",
    "print(nodes[2].metadata['excerpt_keywords'])\n",
    "type(nodes[2].metadata['excerpt_keywords'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "84f5ea44-2abf-4a5f-9654-1b8990982c14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AI',\n",
       " 'AI Genie',\n",
       " 'AI accelerator',\n",
       " 'AI accelerators',\n",
       " 'ASIC',\n",
       " 'Accuracy',\n",
       " 'AdaBoost',\n",
       " 'Ai-Genie',\n",
       " 'Algorithms',\n",
       " 'AlphaGo',\n",
       " 'Analogies',\n",
       " 'Annotations',\n",
       " 'Architecture',\n",
       " 'Attention',\n",
       " 'BERT',\n",
       " 'Bayesian Optimization',\n",
       " 'Bayesian optimization',\n",
       " 'BiGS',\n",
       " 'Bias',\n",
       " 'Black Box',\n",
       " 'Black Box Optimization',\n",
       " 'Byte Pair Encoding',\n",
       " 'CPU',\n",
       " 'CPUs',\n",
       " 'Categorical Crossentropy',\n",
       " 'ChatGPT',\n",
       " 'ChatGPT-4o',\n",
       " 'Classification',\n",
       " 'Coding',\n",
       " 'Configuration File',\n",
       " 'Containerization',\n",
       " 'Context Length',\n",
       " 'Convergence Plot',\n",
       " 'Custom Model',\n",
       " 'DNN',\n",
       " 'DNN accelerators',\n",
       " 'DPO',\n",
       " 'DRAM',\n",
       " 'Data',\n",
       " 'Data Analysis',\n",
       " 'Data Annotation',\n",
       " 'Data Augmentation',\n",
       " 'Data Conversion',\n",
       " 'Data Preparation',\n",
       " 'Debiasing',\n",
       " 'Decision Trees',\n",
       " 'Decoder',\n",
       " 'Decoder Blocks',\n",
       " 'Decoding',\n",
       " 'Deep Learning',\n",
       " 'DeepMind',\n",
       " 'Developments',\n",
       " 'Diagrams',\n",
       " 'Discrimination',\n",
       " 'DocBin',\n",
       " 'Docker',\n",
       " 'Dropout',\n",
       " 'Ecommerce',\n",
       " 'Efficiency',\n",
       " 'Embedding',\n",
       " 'Embeddings',\n",
       " 'Encoding',\n",
       " 'Ensemble Methods',\n",
       " 'Equity',\n",
       " 'Evaluation',\n",
       " 'F-Score',\n",
       " 'F-score',\n",
       " 'FastAPI',\n",
       " 'Feed Forward',\n",
       " 'Feedback',\n",
       " 'Feynman Technique',\n",
       " 'Fully Connected Network',\n",
       " 'GELU',\n",
       " 'GEMM',\n",
       " 'GPT-2',\n",
       " 'GPTBlock',\n",
       " 'GPU',\n",
       " 'GPUs',\n",
       " 'Gaussian Process',\n",
       " 'Gaussian process',\n",
       " 'Gender Bias',\n",
       " 'Generate',\n",
       " 'GitHub',\n",
       " 'GitHub Repo',\n",
       " 'Github',\n",
       " 'Github Repo',\n",
       " 'Global Minimum',\n",
       " 'Google',\n",
       " 'Google Colab',\n",
       " 'Google TPU',\n",
       " 'HBM',\n",
       " 'Here are 10 unique keywords for the document',\n",
       " \"Here's a freshman-level summary of the article\",\n",
       " 'Hugging Face',\n",
       " 'HuggingFace',\n",
       " 'HumanEval',\n",
       " 'Indexing',\n",
       " 'Inference',\n",
       " 'Input Stationary',\n",
       " 'Intersectionality',\n",
       " 'Keys',\n",
       " 'Keywords: AI',\n",
       " 'Keywords: AI accelerator',\n",
       " 'Keywords: AdaBoost',\n",
       " 'Keywords: Bayesian optimization',\n",
       " 'Keywords: Feynman Technique',\n",
       " 'Keywords: GPT-2',\n",
       " 'Keywords: Language Models',\n",
       " 'Keywords: Machine Learning',\n",
       " 'Keywords: NLP',\n",
       " 'Keywords: xLSTM',\n",
       " 'Knowledge Graph',\n",
       " 'Kubectl',\n",
       " 'Kubernetes',\n",
       " 'LLM',\n",
       " 'LLMs',\n",
       " 'LSTM',\n",
       " 'Language Model',\n",
       " 'Large Language Models',\n",
       " 'LayerNorm',\n",
       " 'Learning Rate',\n",
       " 'Linear Layer',\n",
       " 'Local Minimum',\n",
       " 'MAC',\n",
       " 'MAC unit',\n",
       " 'MAC units',\n",
       " 'MBPP',\n",
       " 'MLops',\n",
       " 'MXU',\n",
       " 'Machine Learning',\n",
       " 'Memory Cells',\n",
       " 'Metadata',\n",
       " 'Minikube',\n",
       " 'Mobile Phone',\n",
       " 'Model',\n",
       " 'Model Deployment',\n",
       " 'Model Training',\n",
       " 'Multi-Head Attention',\n",
       " 'Multi-token Prediction',\n",
       " 'NER',\n",
       " 'NLP',\n",
       " 'Named Entity Recognition',\n",
       " 'Natural Language Processing',\n",
       " 'Neural Networks',\n",
       " 'Newsletter',\n",
       " 'Objective Function',\n",
       " 'OpenAI',\n",
       " 'Optimization',\n",
       " 'Output Stationary',\n",
       " 'Overfitting',\n",
       " 'PPO',\n",
       " 'Pandas',\n",
       " 'Pandemic',\n",
       " 'Parallelization',\n",
       " 'Parameter Sharing',\n",
       " 'Parameters',\n",
       " 'Positional Encoding',\n",
       " 'Practical Applications',\n",
       " 'Precision',\n",
       " 'Product Categorization',\n",
       " 'Product Description',\n",
       " 'PyTorch',\n",
       " 'Python',\n",
       " 'Queries',\n",
       " 'Racial Bias',\n",
       " 'Real',\n",
       " 'Recall',\n",
       " 'Research',\n",
       " 'Results',\n",
       " 'Retrieval',\n",
       " 'Richard Feynman',\n",
       " 'SKOPT',\n",
       " 'SRAM',\n",
       " 'Scalability',\n",
       " 'SciKit-learn',\n",
       " 'Scikit-learn',\n",
       " 'Search Space',\n",
       " 'Self-Attention',\n",
       " 'Sentiment Analysis',\n",
       " 'Societal Bias',\n",
       " 'Society',\n",
       " 'SpaCy',\n",
       " 'Spacy',\n",
       " 'Span Identification',\n",
       " 'Span Prediction',\n",
       " 'State-Space Models',\n",
       " 'Stereotypes',\n",
       " 'TED',\n",
       " 'TED Talks',\n",
       " 'TPU',\n",
       " 'TPUs',\n",
       " 'Technology',\n",
       " 'Testing',\n",
       " 'Thinc',\n",
       " 'Tiktoken',\n",
       " 'Tok2Vec',\n",
       " 'Tok2vec',\n",
       " 'Token Embedding',\n",
       " 'Token Generation',\n",
       " 'Tokenization',\n",
       " 'Tokenizer',\n",
       " 'Tokens',\n",
       " 'Torch Compile',\n",
       " 'Training',\n",
       " 'Training Data',\n",
       " 'Training Loop',\n",
       " 'Transcripts',\n",
       " 'Transformer',\n",
       " 'Transformers',\n",
       " 'Trends',\n",
       " 'User Input',\n",
       " 'Uvicorn',\n",
       " 'Vaibhaw Khemka',\n",
       " 'Values,',\n",
       " 'Vector Databases',\n",
       " 'Visualization',\n",
       " 'Vocab_size',\n",
       " 'Vocabulary',\n",
       " 'Vocabulary Size',\n",
       " 'Weak Learner',\n",
       " 'Weight Stationary',\n",
       " 'Word Embeddings',\n",
       " 'YouTube',\n",
       " 'accuracy',\n",
       " 'acquisition function',\n",
       " 'adaptive boosting',\n",
       " 'add examples',\n",
       " 'algorithm',\n",
       " 'algorithms',\n",
       " 'alignment',\n",
       " 'alpha',\n",
       " 'analysis',\n",
       " 'and use AI to help you explain it better!**',\n",
       " 'annotation',\n",
       " 'applications',\n",
       " 'bandwidth',\n",
       " 'based on the provided text:\\n\\nKeywords: Named Entity Recognition',\n",
       " 'bias',\n",
       " 'black box',\n",
       " 'boosting',\n",
       " 'business',\n",
       " 'cache hierarchy',\n",
       " 'chip utilization',\n",
       " 'classification',\n",
       " 'combining learners',\n",
       " 'complex concepts',\n",
       " 'complicated words.\\n2.  **Find the Holes:** After you try to explain it',\n",
       " 'compute',\n",
       " 'computer architecture',\n",
       " 'concept',\n",
       " 'convergence',\n",
       " 'convergence plot',\n",
       " 'convolution',\n",
       " 'd_model',\n",
       " 'data',\n",
       " 'data preparation',\n",
       " 'dataflow',\n",
       " 'dataset',\n",
       " 'decision boundary',\n",
       " 'decision tree',\n",
       " 'deep learning',\n",
       " 'deep neural networks',\n",
       " 'digestible',\n",
       " 'discrimination',\n",
       " 'diversity',\n",
       " 'domain-specific',\n",
       " 'education',\n",
       " 'ensemble',\n",
       " 'ensemble learning',\n",
       " 'epochs',\n",
       " 'ethics',\n",
       " 'evaluation',\n",
       " 'evolution',\n",
       " 'explanation',\n",
       " 'fine-tuning',\n",
       " 'focusing on the core ideas:\\n\\n**The Feynman Technique: Learn Anything by Explaining It Simply**\\n\\nThis article talks about a powerful learning method called the Feynman Technique',\n",
       " 'governance',\n",
       " 'gp_minimize',\n",
       " 'guide',\n",
       " 'half-precision',\n",
       " 'healthcare',\n",
       " 'heterogeneous computing',\n",
       " 'history',\n",
       " 'hyper-parameters',\n",
       " 'hyperparameter tuning',\n",
       " 'indexing',\n",
       " 'individuals',\n",
       " 'inference',\n",
       " 'input',\n",
       " 'input stationary',\n",
       " 'interconnect',\n",
       " 'intuitive',\n",
       " 'iteration',\n",
       " 'knowledge gaps',\n",
       " 'learning',\n",
       " 'like ChatGPT',\n",
       " \"like you're talking to a child. Avoid big\",\n",
       " 'loss',\n",
       " 'machine learning',\n",
       " 'matrix multiplication',\n",
       " 'matrix unit',\n",
       " 'memory',\n",
       " 'memory capacity',\n",
       " 'memory wall',\n",
       " 'methodology',\n",
       " 'minimum value',\n",
       " 'model',\n",
       " 'model pruning',\n",
       " 'model size',\n",
       " 'n_heads',\n",
       " 'n_layers',\n",
       " \"named after the famous physicist Richard Feynman. The main idea is: **If you can't explain something simply\",\n",
       " 'objective function',\n",
       " 'optimal points',\n",
       " 'optimization',\n",
       " 'optimization algorithm',\n",
       " 'optimizer',\n",
       " 'or a story to make it even easier to understand and remember.\\n\\n**AI and the Feynman Technique**\\n\\nThe article suggests using AI',\n",
       " 'organizations',\n",
       " 'output',\n",
       " 'output stationary',\n",
       " 'overfitting',\n",
       " 'parallel processing',\n",
       " 'parallelization',\n",
       " 'parameters',\n",
       " 'performance',\n",
       " 'performance analysis',\n",
       " 'pictures',\n",
       " 'practical applications',\n",
       " 'quantization',\n",
       " 'quantum electrodynamics',\n",
       " 'reinforcement learning',\n",
       " 'retrieval',\n",
       " 'scalability',\n",
       " 'scaling',\n",
       " 'search space',\n",
       " 'skopt',\n",
       " 'societal impacts',\n",
       " 'supercomputer',\n",
       " 'surrogate model',\n",
       " 'systolic array',\n",
       " 'technology',\n",
       " 'tensor processing unit',\n",
       " 'think about where you stumbled. What parts were confusing? What did you leave out?\\n3.  **Fix It:** Go back and improve your explanation. Make it clearer and fill in the gaps you found. Keep refining it until it makes sense.\\n4.  **Tell a Story:** Once you understand the basics',\n",
       " 'to help with this process. You can use AI to:\\n\\n*   **Get a basic understanding:** Read about the topic.\\n*   **Create a simple explanation:** Write a basic explanation of the concept.\\n*   **Get feedback:** Use AI to help identify gaps in your explanation.\\n*   **Refine your explanation:** Use AI to help improve your explanation.\\n\\n**In short: Learn by explaining',\n",
       " 'training',\n",
       " 'transformer',\n",
       " 'transformer models',\n",
       " 'trends',\n",
       " 'trials',\n",
       " 'vector database',\n",
       " 'verification',\n",
       " 'videos',\n",
       " 'visionaries',\n",
       " 'vocab_size',\n",
       " 'weak learner',\n",
       " 'weak learners',\n",
       " 'weight adjustment',\n",
       " 'weight stationary',\n",
       " 'weighted error',\n",
       " 'xLSTM',\n",
       " \"you don't really understand it.**\\n\\nHere's how it works:\\n\\n1.  **Teach It:** Imagine you're explaining the concept to someone who knows nothing about it. Use simple language\"}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords = set()\n",
    "for node in nodes:\n",
    "    keywords |= set(node.metadata['excerpt_keywords'].split(\", \"))\n",
    "keywords    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f9e65daf-a310-454b-9957-e6df0f77a79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.vector_stores import MetadataFilter, MetadataFilters, FilterOperator\n",
    "\n",
    "filters = MetadataFilters(\n",
    "    filters=[\n",
    "        MetadataFilter(\n",
    "            key=\"excerpt_keywords\",\n",
    "            operator=FilterOperator.TEXT_MATCH,\n",
    "            value='Hugging Face',\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "137d4556-5031-4711-aad5-4cef3f81789b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# Create the index based on the vector store.\n",
    "index = VectorStoreIndex.from_vector_store(vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c44e850d-5404-43da-b23d-8f95ef336646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A basic BERT model from the Hugging Face transformer was used.\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a query engine that is responsible for retrieving related pieces of text,\n",
    "# and using a LLM to formulate the final answer.\n",
    "query_engine = index.as_query_engine(filters=filters)\n",
    "\n",
    "res = query_engine.query(\"What is BERT?\")\n",
    "res.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "28af5c7a-8df1-47bc-9f3b-8d448ec4cfb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node ID\t 180cab3a-f1dc-48f4-b60c-43a4bf9f35e1\n",
      "Title\t BERT HuggingFace Model Deployment using Kubernetes [ Github Repo]  03/07/2024\n",
      "Text\t Github Repo : https://github.com/vaibhawkhemka/ML-Umbrella/tree/main/MLops/Model_Deployment/Bert_Kubernetes_deployment   Model development is useless if you dont deploy it to production  which comes with a lot of issues of scalability and portability.   I have deployed a basic BERT model from the huggingface transformer on Kubernetes with the help of docker  which will give a feel of how to deploy and manage pods on production.   Model Serving and Deployment:ML Pipeline:Workflow:   Model server (using FastAPI  uvicorn) for BERT uncased model    Containerize model and inference scripts to create a docker image    Kubernetes deployment for these model servers (for scalability)  Testing   Components:Model serverUsed BERT uncased model from hugging face for prediction of next word [MASK]. Inference is done using transformer-cli which uses fastapi and uvicorn to serve the model endpoints   Server streaming:   Testing: (fastapi docs)   http://localhost:8888/docs/   { output: [ { score: 0.21721847355365753  token: 2204  token_str: good  sequence: today is a good day }  { score: 0.16623663902282715  token: 2047  token_str: new  sequence: today is a new day }  { score: 0.07342924177646637  token: 2307  token_str: great  sequence: today is a great day }  { score: 0.0656224861741066  token: 2502  token_str: big  sequence: today is a big day }  { score: 0.03518620505928993  token: 3376  token_str: beautiful  sequence: today is a beautiful day } ]   ContainerizationCreated a docker image from huggingface GPU base image and pushed to dockerhub after testing.   Testing on docker container:   You can directly pull the image vaibhaw06/bert-kubernetes:latest   K8s deploymentUsed minikube and kubectl commands to create a single pod container for serving the model by configuring deployment and service config\n",
      "Score\t 0.6188017496400591\n",
      "Score\t BERT, Hugging Face, Kubernetes, Docker, Model Deployment, FastAPI, Uvicorn, Minikube, Scalability, Containerization\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n"
     ]
    }
   ],
   "source": [
    "# Show the retrieved nodes\n",
    "for src in res.source_nodes:\n",
    "    print(\"Node ID\\t\", src.node_id)\n",
    "    print(\"Title\\t\", src.metadata[\"title\"])\n",
    "    print(\"Text\\t\", src.text)\n",
    "    print(\"Score\\t\", src.score)\n",
    "    print(\"Score\\t\", src.metadata[\"excerpt_keywords\"])\n",
    "    print(\"-_\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e96bd53-d46d-4dde-8f23-4810c5d5549b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
