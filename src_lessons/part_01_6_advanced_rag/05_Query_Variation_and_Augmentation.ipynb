{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "170bd55d-5844-48cf-b54c-a2d1e94fd6b3",
   "metadata": {},
   "source": [
    "# Query Variation and Augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6b5405ea-c793-4553-91f1-8bcc293b1c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.core.query_engine.multistep_query_engine import MultiStepQueryEngine\n",
    "from llama_index.llms.google_genai import GoogleGenAI\n",
    "from llama_index.embeddings.google_genai import GoogleGenAIEmbedding\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.indices.query.query_transform.base import (\n",
    "    StepDecomposeQueryTransform,\n",
    ")\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from typing import Any, List, Optional, Sequence, Union\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "import asyncio\n",
    "import chromadb\n",
    "import logging as log\n",
    "import os\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "49eb6c33-11e2-4078-b531-1adff20d892d",
   "metadata": {},
   "outputs": [],
   "source": [
    "FORMAT_STRING = \"%(module)s.%(funcName)s():%(lineno)d %(asctime)s\\n[%(levelname)-5s] %(message)s\\n\"\n",
    "log.basicConfig(level= log.ERROR, format=FORMAT_STRING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "53b48e6e-4cd6-47c5-9ad5-1e53204ec86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "67721d08-439b-4fb3-bde8-802af28c5f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_API_KEY_NAME='GOOGLE_API_KEY'\n",
    "assert GOOGLE_API_KEY_NAME in os.environ\n",
    "\n",
    "OPEN_AI_KEY_NAME='OPENAI_API_KEY'\n",
    "assert OPEN_AI_KEY_NAME in os.environ\n",
    "\n",
    "TAI_DATASET_ROOT_ENV_VAR='TAI_DATASET_ROOT'\n",
    "assert TAI_DATASET_ROOT_ENV_VAR in os.environ\n",
    "\n",
    "model_name= \"gemini-2.0-flash-lite\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "78508119-112d-4b38-b68b-78713528f13b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_client._send_single_request():1025 2025-04-06 17:16:35,321\n",
      "[INFO ] HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-lite \"HTTP/1.1 200 OK\"\n",
      "\n",
      "models.generate_content():4934 2025-04-06 17:16:35,331\n",
      "[INFO ] AFC is enabled with max remote calls: 10.\n",
      "\n",
      "_client._send_single_request():1025 2025-04-06 17:16:40,441\n",
      "[INFO ] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-lite:generateContent \"HTTP/1.1 200 OK\"\n",
      "\n",
      "models.generate_content():4945 2025-04-06 17:16:40,447\n",
      "[INFO ] AFC remote call 1 is done.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Richard Feynman (1918-1988) was a highly influential American theoretical physicist. He is widely regarded as one of the most brilliant and original minds of the 20th century. Here's a breakdown of his key contributions and characteristics:\n",
      "\n",
      "**Key Contributions to Physics:**\n",
      "\n",
      "*   **Quantum Electrodynamics (QED):** Feynman is best known for his work on QED, the theory describing the interaction of light and matter. He developed a groundbreaking approach using:\n",
      "    *   **Feynman Diagrams:** These \n",
      "[...]\n"
     ]
    }
   ],
   "source": [
    "llm = GoogleGenAI(model=model_name)\n",
    "response = llm.complete('Who is Richard Feynman')\n",
    "print(str(response)[:500]+ '\\n[...]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "92d6f18c-b847-47a1-a0fa-8b948ef47ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Vector database was created using the OpenAI embedding !!!!!!\n",
    "\n",
    "# Settings.llm = GoogleGenAI(model=model_name)\n",
    "# Settings.embed_model = GoogleGenAIEmbedding(model=\"text-embedding-3-small\")\n",
    "\n",
    "Settings.llm = OpenAI(temperature=0, model=\"gpt-4o-mini\")\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a85698af-6c21-4135-9c49-1d762d8c841b",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_decompose_transform = StepDecomposeQueryTransform(verbose=True, llm=Settings.llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4b2f9718-c723-4f1d-9031-627c4c48ca1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorDbPath: /home/minguzzi/repo/towards_ai_course/dataset/ai_tutor_knowledge_vectdb\n"
     ]
    }
   ],
   "source": [
    "assert TAI_DATASET_ROOT_NAME in os.environ\n",
    "vectorDbPath = os.path.join( os.environ[TAI_DATASET_ROOT_NAME], 'ai_tutor_knowledge_vectdb')\n",
    "print(f'vectorDbPath: {vectorDbPath}')\n",
    "\n",
    "# Load the vector store from the local storage.\n",
    "db = chromadb.PersistentClient(path=vectorDbPath)\n",
    "chroma_collection = db.get_collection(\"ai_tutor_knowledge\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6fa73dbc-e6d4-4355-af05-faea05d1bd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index = VectorStoreIndex.from_vector_store(vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "517e66a3-7aaa-4b02-8493-32c9d7724e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_client._send_single_request():1025 2025-04-06 17:23:45,296\n",
      "[INFO ] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;33m> Current query: Write about Llama 3.1 Model, BERT and PEFT methods\n",
      "\u001b[0m\u001b[1;3;38;5;200m> New query: What are the key features of the Llama 3.1 Model?\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_client._send_single_request():1025 2025-04-06 17:23:46,013\n",
      "[INFO ] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "\n",
      "_client._send_single_request():1025 2025-04-06 17:23:50,212\n",
      "[INFO ] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "_client._send_single_request():1025 2025-04-06 17:23:51,338\n",
      "[INFO ] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;33m> Current query: Write about Llama 3.1 Model, BERT and PEFT methods\n",
      "\u001b[0m\u001b[1;3;38;5;200m> New query: What are the key features of the Llama 3.1 Model?\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_client._send_single_request():1025 2025-04-06 17:23:57,072\n",
      "[INFO ] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "\n",
      "_client._send_single_request():1025 2025-04-06 17:24:01,475\n",
      "[INFO ] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "_client._send_single_request():1025 2025-04-06 17:24:02,806\n",
      "[INFO ] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;33m> Current query: Write about Llama 3.1 Model, BERT and PEFT methods\n",
      "\u001b[0m\u001b[1;3;38;5;200m> New query: What are the key features of the BERT model?\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_client._send_single_request():1025 2025-04-06 17:24:05,264\n",
      "[INFO ] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "\n",
      "_client._send_single_request():1025 2025-04-06 17:24:08,950\n",
      "[INFO ] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "_client._send_single_request():1025 2025-04-06 17:24:16,937\n",
      "[INFO ] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Llama 3.1 model is a state-of-the-art AI language model developed by Meta, featuring significant advancements in scale and capabilities. It is the largest model from Meta, trained on over 15 trillion tokens using more than 16,000 H100 GPUs. One of its standout features is the extended context length of 128K, which allows it to process longer text inputs effectively. The model excels in reasoning and coding tasks, demonstrating strong logical reasoning, problem-solving, and analytical skills. Additionally, it supports zero-shot tool use, enabling it to perform tasks without prior training on specific tools. With approximately 50% of its training data consisting of multilingual tokens, Llama 3.1 is adept at understanding and processing multiple languages. It also outperforms competing models in various benchmarks, particularly in mathematical reasoning and complex reasoning tasks.\n",
      "\n",
      "BERT, on the other hand, is a foundational model in natural language processing characterized by its bidirectional processing capabilities. It utilizes an encoder-only architecture, which allows it to generate contextualized word representations by considering the entire context of a sentence. BERT is pre-trained on two main tasks: Masked Language Modeling (MLM) and Next Sentence Prediction (NSP), which enhance its understanding of word meanings and sentence relationships. It comes in two variants, BERT BASE and BERT LARGE, differing in the number of layers and parameters.\n",
      "\n",
      "PEFT, or Parameter-Efficient Fine-Tuning, is a method designed to adapt large pre-trained models like Llama 3.1 and BERT for specific tasks without the need for extensive retraining. This approach focuses on fine-tuning only a small subset of parameters or adding lightweight adapters, making it resource-efficient and faster while maintaining the model's performance on downstream tasks. PEFT methods are particularly useful in scenarios where computational resources are limited or when rapid deployment is necessary. \n",
      "\n",
      "Together, these models and methods represent significant advancements in the field of AI and natural language processing, each contributing unique strengths to the development of intelligent systems.\n"
     ]
    }
   ],
   "source": [
    "# Multi Step Query Engine\n",
    "query_engine_from_vector_db = vector_index.as_query_engine()\n",
    "multi_step_query_engine = MultiStepQueryEngine(\n",
    "    query_engine = query_engine_from_vector_db,\n",
    "    query_transform = step_decompose_transform,\n",
    "    index_summary = \"\"\"Used to answer the Questions about RAG, Machine Learning, Deep Learning, and Generative AI, Note: Don't absolutely repeat the Same quesion, for any reason.\"\"\",\n",
    ")\n",
    "\n",
    "response = multi_step_query_engine.query(\"Write about Llama 3.1 Model, BERT and PEFT methods\")\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7610b513-4dfe-46b1-9a3c-6b82a699005c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_client._send_single_request():1025 2025-04-06 17:20:28,893\n",
      "[INFO ] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;33m> Current query: Write about Llama 3.1 Model, BERT and PEFT methods\n",
      "\u001b[0m\u001b[1;3;38;5;200m> New query: What are the key features of the Llama 3.1 Model?\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_client._send_single_request():1025 2025-04-06 17:20:32,376\n",
      "[INFO ] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "\n",
      "_client._send_single_request():1025 2025-04-06 17:20:36,472\n",
      "[INFO ] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "_client._send_single_request():1025 2025-04-06 17:20:37,905\n",
      "[INFO ] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;33m> Current query: Write about Llama 3.1 Model, BERT and PEFT methods\n",
      "\u001b[0m\u001b[1;3;38;5;200m> New query: What are the key features of the Llama 3.1 Model?\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_client._send_single_request():1025 2025-04-06 17:20:39,954\n",
      "[INFO ] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "\n",
      "_client._send_single_request():1025 2025-04-06 17:20:44,357\n",
      "[INFO ] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "_client._send_single_request():1025 2025-04-06 17:20:44,971\n",
      "[INFO ] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;33m> Current query: Write about Llama 3.1 Model, BERT and PEFT methods\n",
      "\u001b[0m\u001b[1;3;38;5;200m> New query: What are the key features of BERT?\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_client._send_single_request():1025 2025-04-06 17:20:47,429\n",
      "[INFO ] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "\n",
      "_client._send_single_request():1025 2025-04-06 17:20:51,217\n",
      "[INFO ] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "_client._send_single_request():1025 2025-04-06 17:21:00,228\n",
      "[INFO ] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Llama 3.1 model is a state-of-the-art AI language model developed by Meta, featuring a large scale with training on over 15 trillion tokens using more than 16,000 H100 GPUs. It supports an extended context length of 128K, allowing it to process longer inputs effectively. The model excels in reasoning and coding capabilities, generating high-quality code and demonstrating strong problem-solving skills. It also supports zero-shot tool use, enabling it to perform tasks without prior training on specific tools. With approximately 50% of its training data being multilingual, Llama 3.1 is adept at understanding and processing multiple languages. Its benchmark performance surpasses that of competing models in areas such as mathematical reasoning and long text processing.\n",
      "\n",
      "BERT, or Bidirectional Encoder Representations from Transformers, is another influential model characterized by its bidirectional processing, which captures the full context of sentences by analyzing text in both directions simultaneously. It employs an encoder-only transformer architecture to generate contextualized word embeddings. BERT is pre-trained on two main tasks: Masked Language Modeling (MLM) and Next Sentence Prediction (NSP), which enhance its understanding of word relationships and sentence structures. It comes in two variants, BERT BASE and BERT LARGE, differing in the number of layers and parameters.\n",
      "\n",
      "PEFT, or Parameter-Efficient Fine-Tuning, is a method designed to adapt pre-trained models like Llama 3.1 and BERT for specific tasks with minimal additional parameters. This approach allows for efficient fine-tuning by focusing on a small subset of parameters, making it resource-efficient and effective for various applications. PEFT methods are particularly useful in scenarios where computational resources are limited or when rapid adaptation to new tasks is required.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cdd348b2-d5ac-452b-bdf1-57608eb46d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY:\n",
      "What are the key features of the Llama 3.1 Model?\n",
      "\n",
      "RESPONSE:\n",
      "The Llama 3.1 model boasts several key features, including:\n",
      "\n",
      "1. **Model Scale and Training**: It is the largest model from Meta, trained on over 15 trillion tokens using more than 16,000 H100 GPUs.\n",
      "\n",
      "2. **Extended Context Length**: The model supports a context length of 128K, enhancing its ability to handle longer inputs.\n",
      "\n",
      "3. **Enhanced Capabilities**: It demonstrates improved reasoning and coding abilities compared to its predecessors.\n",
      "\n",
      "4. **Tool Use and Agentic Behaviors**: The model supports zero-shot tool use, allowing it to perform tasks without prior training on specific tools.\n",
      "\n",
      "5. **Multilingual Processing**: Approximately 50% of its training data consists of multilingual tokens, enabling effective understanding and processing of multiple languages.\n",
      "\n",
      "6. **Programming and Reasoning Skills**: Llama 3.1 excels in generating high-quality code and exhibits strong logical reasoning, problem-solving, and analytical capabilities.\n",
      "\n",
      "7. **Benchmark Performance**: It outperforms competing models in areas such as mathematical reasoning, complex reasoning, multilingual support, and long text processing.\n",
      "\n",
      "These features collectively position Llama 3.1 as a powerful tool in the landscape of AI language models.\n",
      "\n",
      "QUERY:\n",
      "What are the key features of the Llama 3.1 Model?\n",
      "\n",
      "RESPONSE:\n",
      "The Llama 3.1 model boasts several key features, including:\n",
      "\n",
      "1. **Large Scale and Training**: It is Meta's largest model, trained on over 15 trillion tokens using more than 16,000 H100 GPUs.\n",
      "\n",
      "2. **Extended Context Length**: The model supports a context length of 128K, enhancing its ability to handle extensive text inputs.\n",
      "\n",
      "3. **Enhanced Reasoning and Coding Capabilities**: Llama 3.1 excels in generating high-quality code and demonstrates strong logical reasoning, problem-solving, and analytical skills.\n",
      "\n",
      "4. **Multilingual Processing**: Approximately 50% of its training data consists of multilingual tokens, allowing it to effectively understand and process multiple languages.\n",
      "\n",
      "5. **Zero-shot Tool Use**: The model supports zero-shot tool use and can develop agentic behaviors, showcasing its versatility in various applications.\n",
      "\n",
      "6. **Benchmark Performance**: It outperforms competing models like GPT-4o and Claude 3.5 in areas such as mathematical reasoning, complex reasoning, multilingual support, and long text processing.\n",
      "\n",
      "7. **Multimodal Capabilities**: While still in development, there are plans for multimodal capabilities that will include image, video, and speech recognition.\n",
      "\n",
      "These features collectively position Llama 3.1 as a powerful tool in the landscape of AI language models.\n",
      "\n",
      "QUERY:\n",
      "What are the key features of BERT?\n",
      "\n",
      "RESPONSE:\n",
      "BERT, or Bidirectional Encoder Representations from Transformers, is characterized by several key features:\n",
      "\n",
      "1. **Bidirectional Processing**: BERT processes text in both directions simultaneously, allowing it to capture the full context of a sentence. This contrasts with traditional unidirectional models that read text sequentially, which can lead to ambiguity in understanding.\n",
      "\n",
      "2. **Transformer Architecture**: It utilizes an encoder-only transformer architecture, which is designed to generate contextualized word embeddings by attending to all tokens in a sequence.\n",
      "\n",
      "3. **Pre-training Tasks**: BERT is pre-trained on two main tasks:\n",
      "   - **Masked Language Modeling (MLM)**: This involves predicting masked tokens in sentences, helping the model learn contextual relationships between words.\n",
      "   - **Next Sentence Prediction (NSP)**: This task involves predicting whether one sentence is likely to follow another, enhancing the model's understanding of sentence relationships.\n",
      "\n",
      "4. **Model Variants**: BERT comes in two versions: BERT BASE, which has 12 layers and 110 million parameters, and BERT LARGE, which has 24 layers and 340 million parameters.\n",
      "\n",
      "These features collectively contribute to BERT's effectiveness in understanding and generating human-like language.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for query, resp in response.metadata['sub_qa']:\n",
    "    print(f\"QUERY:\\n{query}\\n\\nRESPONSE:\\n{resp}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "edfbcd27-59fa-421c-9eca-bb1438aaf426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node ID\t d3fdc971-e094-4ae2-b348-7400cc2f2fe5\n",
      "Text\t \n",
      "Question: What are the key features of the Llama 3.1 Model?\n",
      "Answer: The Llama 3.1 model boasts several key features, including:\n",
      "\n",
      "1. **Model Scale and Training**: It is the largest model from Meta, trained on over 15 trillion tokens using more than 16,000 H100 GPUs.\n",
      "\n",
      "2. **Extended Context Length**: The model supports a context length of 128K, enhancing its ability to handle longer inputs.\n",
      "\n",
      "3. **Enhanced Capabilities**: It demonstrates improved reasoning and coding abilities compared to its predecessors.\n",
      "\n",
      "4. **Tool Use and Agentic Behaviors**: The model supports zero-shot tool use, allowing it to perform tasks without prior training on specific tools.\n",
      "\n",
      "5. **Multilingual Processing**: Approximately 50% of its training data consists of multilingual tokens, enabling effective understanding and processing of multiple languages.\n",
      "\n",
      "6. **Programming and Reasoning Skills**: Llama 3.1 excels in generating high-quality code and exhibits strong logical reasoning, problem-solving, and analytical capabilities.\n",
      "\n",
      "7. **Benchmark Performance**: It outperforms competing models in areas such as mathematical reasoning, complex reasoning, multilingual support, and long text processing.\n",
      "\n",
      "These features collectively position Llama 3.1 as a powerful tool in the landscape of AI language models.\n",
      "Score\t None\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "Node ID\t 36015f39-2a42-40b7-b6fa-ec45e9fe3fe2\n",
      "Text\t \n",
      "Question: What are the key features of the Llama 3.1 Model?\n",
      "Answer: The Llama 3.1 model boasts several key features, including:\n",
      "\n",
      "1. **Large Scale and Training**: It is Meta's largest model, trained on over 15 trillion tokens using more than 16,000 H100 GPUs.\n",
      "\n",
      "2. **Extended Context Length**: The model supports a context length of 128K, enhancing its ability to handle extensive text inputs.\n",
      "\n",
      "3. **Enhanced Reasoning and Coding Capabilities**: Llama 3.1 excels in generating high-quality code and demonstrates strong logical reasoning, problem-solving, and analytical skills.\n",
      "\n",
      "4. **Multilingual Processing**: Approximately 50% of its training data consists of multilingual tokens, allowing it to effectively understand and process multiple languages.\n",
      "\n",
      "5. **Zero-shot Tool Use**: The model supports zero-shot tool use and can develop agentic behaviors, showcasing its versatility in various applications.\n",
      "\n",
      "6. **Benchmark Performance**: It outperforms competing models like GPT-4o and Claude 3.5 in areas such as mathematical reasoning, complex reasoning, multilingual support, and long text processing.\n",
      "\n",
      "7. **Multimodal Capabilities**: While still in development, there are plans for multimodal capabilities that will include image, video, and speech recognition.\n",
      "\n",
      "These features collectively position Llama 3.1 as a powerful tool in the landscape of AI language models.\n",
      "Score\t None\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "Node ID\t d76c8b10-e67a-481f-8723-d101710dc471\n",
      "Text\t \n",
      "Question: What are the key features of BERT?\n",
      "Answer: BERT, or Bidirectional Encoder Representations from Transformers, is characterized by several key features:\n",
      "\n",
      "1. **Bidirectional Processing**: BERT processes text in both directions simultaneously, allowing it to capture the full context of a sentence. This contrasts with traditional unidirectional models that read text sequentially, which can lead to ambiguity in understanding.\n",
      "\n",
      "2. **Transformer Architecture**: It utilizes an encoder-only transformer architecture, which is designed to generate contextualized word embeddings by attending to all tokens in a sequence.\n",
      "\n",
      "3. **Pre-training Tasks**: BERT is pre-trained on two main tasks:\n",
      "   - **Masked Language Modeling (MLM)**: This involves predicting masked tokens in sentences, helping the model learn contextual relationships between words.\n",
      "   - **Next Sentence Prediction (NSP)**: This task involves predicting whether one sentence is likely to follow another, enhancing the model's understanding of sentence relationships.\n",
      "\n",
      "4. **Model Variants**: BERT comes in two versions: BERT BASE, which has 12 layers and 110 million parameters, and BERT LARGE, which has 24 layers and 340 million parameters.\n",
      "\n",
      "These features collectively contribute to BERT's effectiveness in understanding and generating human-like language.\n",
      "Score\t None\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "Node ID\t f5453fa3-1b20-4c2e-8549-9882cc954df3\n",
      "Text\t Llama 3.1 models  especially the 405 billion parameter version (also the 70B  8B)?   For the 405B parameter version  substantial GPU resources are required  up to 16K H100 GPUs for training  with 80GB HBM3 memory each  connected via NVLink within servers equipped with eight GPUs and two CPUs. Smaller versions (70B  8B) have lower resource requirements  using Nvidia Quantum2 InfiniBand fabric with 400 Gbps interconnects between GPUs  making them more accessible for many organizations  while storage requirements include a distributed file system offering up to 240 PB of storage with a peak throughput of 7 TB/s. Recently  Elie Bakouch (known for training LLMs on Hugging Face) shared that one can fine-tune Llama 3 405B using 8 H100 GPUs.   5  What specific advantages does Llama 3.1 offer in terms of performance  cost  and potential cost savings compared to closed models like GPT-4o?   Llama 3.1 offers significant advantages in performance  matching or surpassing GPT-4 in many benchmarks  while being more economical to run  with inference operations costing roughly 50% less than comparable closed models like GPT-4o according to an interview with Mark Zuckerberg. The open-source nature allows for more efficient customization and fine-tuning  potentially leading to better performance on specific tasks at a lower cost compared to closed models  while the ability to run the model on-premises or on preferred cloud providers gives organizations more control over their infrastructure costs.   6  What kind of skills/team does it take to work with Llama models effectively for our specific use cases?   a  For Fine-tuning  training  distilling   A team needs expertise in machine learning  particularly in natural language processing and transformer architectures. Skills in data preprocessing  model optimization  and distributed computing are crucial. Knowledge of PyTorch and experience with large-scale model training is essential. The team should include ML engineers  ML ops specialists  and developers.   b  For Deploying/using out-of-the-box   For deploying and using Llama models out-of-the-box  the necessary skills shift towards software development and cloud services expertise.\n",
      "Score\t 0.4431183134534493\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "Node ID\t fc78a10c-1358-435e-9a68-c21718fac23b\n",
      "Text\t not yet been widely released.   Benchmark ResultsMeta compared the Llama 3.1 405B model with models such as GPT-4  GPT-4o  and Claude 3.5 sonnet. The results showed that Llama 3.1 performed better than GPT-4o and Claude 3.5 sonnet on test data sets such as mathematical reasoning  complex reasoning  and Multilingual support and its long text processing ability is also excellent  receiving 95.2 points in zero scrolls quality.   it falls short compared to Claude 3.5 sonnet in tool utilization ability (BFCL  Nexus).   Although the performance on test data sets such as Multi-task Language Understanding  Human Eval  and MATH is slightly inferior to the closed-source model  the score difference is not large.   In addition  manual evaluation results show that the output performance of the Llama 3.1 405B model is comparable to GPT-4 and Claude 3.5 Sonnet  and slightly inferior to GPT-4o.   Just looking at this benchmark score  it seems to be quite promising.The benchmark results show that llama 3.1 405B is an excellent language model with strong language modeling capabilities  mathematical reasoning capabilities  complex reasoning and long text processing capabilities. however  there is still room for improvement in tool utilization capabilities and multilingual support.   Now that Ive introduced the benchmark scores  Im going to try using them and see how they perform.   How to Use Llama 3.1 40 5B Locally?Ollama is the fastest way to get up and running With local language models We recommend trying Llama 3.1 8b  which is impressive for its size and will perform well on most hardware.   Download Ollama here (it should walk you through the rest of these steps)Open a terminal and run ollama run llama3.1-8bGroq is now hosting the Llama 3.1 models  including the 70B and 8B models. Earlier  they offered the largest 405B model  but it has been temporarily removed due to high traffic and server\n",
      "Score\t 0.4254576442027585\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "Node ID\t f5453fa3-1b20-4c2e-8549-9882cc954df3\n",
      "Text\t Llama 3.1 models  especially the 405 billion parameter version (also the 70B  8B)?   For the 405B parameter version  substantial GPU resources are required  up to 16K H100 GPUs for training  with 80GB HBM3 memory each  connected via NVLink within servers equipped with eight GPUs and two CPUs. Smaller versions (70B  8B) have lower resource requirements  using Nvidia Quantum2 InfiniBand fabric with 400 Gbps interconnects between GPUs  making them more accessible for many organizations  while storage requirements include a distributed file system offering up to 240 PB of storage with a peak throughput of 7 TB/s. Recently  Elie Bakouch (known for training LLMs on Hugging Face) shared that one can fine-tune Llama 3 405B using 8 H100 GPUs.   5  What specific advantages does Llama 3.1 offer in terms of performance  cost  and potential cost savings compared to closed models like GPT-4o?   Llama 3.1 offers significant advantages in performance  matching or surpassing GPT-4 in many benchmarks  while being more economical to run  with inference operations costing roughly 50% less than comparable closed models like GPT-4o according to an interview with Mark Zuckerberg. The open-source nature allows for more efficient customization and fine-tuning  potentially leading to better performance on specific tasks at a lower cost compared to closed models  while the ability to run the model on-premises or on preferred cloud providers gives organizations more control over their infrastructure costs.   6  What kind of skills/team does it take to work with Llama models effectively for our specific use cases?   a  For Fine-tuning  training  distilling   A team needs expertise in machine learning  particularly in natural language processing and transformer architectures. Skills in data preprocessing  model optimization  and distributed computing are crucial. Knowledge of PyTorch and experience with large-scale model training is essential. The team should include ML engineers  ML ops specialists  and developers.   b  For Deploying/using out-of-the-box   For deploying and using Llama models out-of-the-box  the necessary skills shift towards software development and cloud services expertise.\n",
      "Score\t 0.4431183134534493\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "Node ID\t fc78a10c-1358-435e-9a68-c21718fac23b\n",
      "Text\t not yet been widely released.   Benchmark ResultsMeta compared the Llama 3.1 405B model with models such as GPT-4  GPT-4o  and Claude 3.5 sonnet. The results showed that Llama 3.1 performed better than GPT-4o and Claude 3.5 sonnet on test data sets such as mathematical reasoning  complex reasoning  and Multilingual support and its long text processing ability is also excellent  receiving 95.2 points in zero scrolls quality.   it falls short compared to Claude 3.5 sonnet in tool utilization ability (BFCL  Nexus).   Although the performance on test data sets such as Multi-task Language Understanding  Human Eval  and MATH is slightly inferior to the closed-source model  the score difference is not large.   In addition  manual evaluation results show that the output performance of the Llama 3.1 405B model is comparable to GPT-4 and Claude 3.5 Sonnet  and slightly inferior to GPT-4o.   Just looking at this benchmark score  it seems to be quite promising.The benchmark results show that llama 3.1 405B is an excellent language model with strong language modeling capabilities  mathematical reasoning capabilities  complex reasoning and long text processing capabilities. however  there is still room for improvement in tool utilization capabilities and multilingual support.   Now that Ive introduced the benchmark scores  Im going to try using them and see how they perform.   How to Use Llama 3.1 40 5B Locally?Ollama is the fastest way to get up and running With local language models We recommend trying Llama 3.1 8b  which is impressive for its size and will perform well on most hardware.   Download Ollama here (it should walk you through the rest of these steps)Open a terminal and run ollama run llama3.1-8bGroq is now hosting the Llama 3.1 models  including the 70B and 8B models. Earlier  they offered the largest 405B model  but it has been temporarily removed due to high traffic and server\n",
      "Score\t 0.4254576442027585\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "Node ID\t a6ebe22b-5529-4e78-93a6-e7336d031600\n",
      "Text\t GPT-2  GPT-3  and GPT-4  which were decoder-only architectures. Another well-known example is BERT (Bidirectional Encoder Representations from Transformers)  an encoder-only transformer mode used as a component in sentence embedding models.   Lets talk about BERT!BERT stands for Bidirectional Encoder Representations from Transformers. It is a language model by Google that uses a transformer architecture to understand and generate human-like language. BERT is designed to simultaneously process text in both directions  allowing it to capture context more effectively than traditional unidirectional models  which read text sequentially from left to right or right to left.   Example of Bidirectional CapabilityConsider the sentence:    The bank is situated on the _______ of the river.   In a unidirectional model  understanding the blank would primarily rely on the words before it  potentially leading to ambiguity about whether bank refers to a financial institution or the side of a river.   However  BERTs bidirectional approach allows it to use the entire sentences context  including the words before and after the blank. Thus  the missing word is likely related to the river  resulting in a more accurate prediction  such as bank referring to the riverbank rather than a financial institution.   BERT has two versions:   BERT BASE with    Layers: 12Parameters: 110MAttention Heads: 12Hidden Units: 768BERT LARGE with    Layers: 24Parameters: 340MAttention Heads: 12Hidden Units: 1024DYK?BERT was pre-trained on 3.3 Billion words!   What was it pre-trained on? For what?BERT was pre-trained on two tasks:   Masked Language Modeling (MLM):The inputs are sentences that start with a special token called CLS (Classify Token) and end with a SEP (separator token).   Words  tokens (consider)   Around 15% of the input tokens are masked  and the model is trained to predict those masked tokens.   The model learns to produce contextualized vectors based on the surrounding words at this stage. Read the example above and reread this sentence.   Next Sentence Prediction (NSP):In this one  the model predicts if one sentence is likely to follow another.\n",
      "Score\t 0.4773256519412279\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "Node ID\t 5c948017-64fe-4457-9d22-548f03306e1e\n",
      "Text\t # DeBERTa-v2## OverviewThe DeBERTa model was proposed in [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/abs/2006.03654) by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen It is based on Google'sBERT model released in 2018 and Facebook's RoBERTa model released in 2019.It builds on RoBERTa with disentangled attention and enhanced mask decoder training with half of the data used inRoBERTa.The abstract from the paper is the following:*Recent progress in pre-trained neural language models has significantly improved the performance of many naturallanguage processing (NLP) tasks. In this paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT withdisentangled attention) that improves the BERT and RoBERTa models using two novel techniques. The first is thedisentangled attention mechanism, where each word is represented using two vectors that encode its content andposition, respectively, and the attention weights among words are computed using disentangled matrices on theircontents and relative positions. Second, an enhanced mask decoder is used to replace the output softmax layer topredict the masked tokens for model pretraining. We show that these two techniques significantly improve the efficiencyof model pretraining and performance of downstream tasks. Compared to RoBERTa-Large, a DeBERTa model trained on half ofthe training data performs consistently better on a wide range of NLP tasks, achieving improvements on MNLI by +0.9%(90.2% vs. 91.1%), on SQuAD v2.0 by +2.3% (88.4% vs. 90.7%) and RACE by +3.6% (83.2% vs. 86.8%). The DeBERTa code andpre-trained models will be made publicly available at https://github.com/microsoft/DeBERTa.*The following information is visible directly on the [original implementationrepository](https://github.com/microsoft/DeBERTa). DeBERTa v2 is the second version of the DeBERTa model. It includesthe 1.5B model used for the SuperGLUE single-model submission and achieving 89.9, versus human baseline 89.8. You canfind more details about this\n",
      "Score\t 0.4470522957141264\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n"
     ]
    }
   ],
   "source": [
    "for src in response.source_nodes:\n",
    "    print(\"Node ID\\t\", src.node_id)\n",
    "    print(\"Text\\t\", src.text)\n",
    "    print(\"Score\\t\", src.score)\n",
    "    print(\"-_\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e131af-d99d-448d-a1e6-188d244b01c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "104f8b1c-23a1-4f5f-aa5c-303febddf452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<llama_index.core.indices.query.query_transform.base.StepDecomposeQueryTransform object at 0x7fe085a3a5f0>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# StepDecomposeQueryTransform with verbose output\n",
    "other_decompose_transform = StepDecomposeQueryTransform(llm=llm, verbose=True)\n",
    "pprint.pp(other_decompose_transform)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d5fba5-a638-49e1-bbdc-f03a5a69d924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a complex query\n",
    "query = \"How can artificial intelligence be used in healthcare and education?\"\n",
    "\n",
    "# Decompose the query using StepDecomposeQueryTransform\n",
    "decomposed_result = other_decompose_transform.as_query_component().query( query)\n",
    "\n",
    "# Print the decomposed results\n",
    "print(\"Decomposed Query Result:\", decomposed_result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
