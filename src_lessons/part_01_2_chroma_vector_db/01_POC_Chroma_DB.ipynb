{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d149e39-3ac5-4ada-8ac9-6542f984268a",
   "metadata": {},
   "source": [
    "# Towards AI Online Course: Chroma DB - Vector Database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ee402a3-25a0-4b5b-99b8-8d0010c396e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import csv\n",
    "import logging as log\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efecd91f-e709-46b4-ae5b-bc39e23d7a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "log.basicConfig(level=log.INFO, format='[%(levelname)5s] %(asctime)s: %(message)s',\n",
    "                  datefmt='%H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47de572a-0d64-4a28-9372-1e6355a8a8c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ INFO] 14:43:11: miniArticlesFilepath: /home/minguzzi/repo/towards_ai_course/dataset/rag_ai_tutor/mini-llama-articles.txt\n"
     ]
    }
   ],
   "source": [
    "os.environ['TAI_DATASET_ROOT']\n",
    "assert 'TAI_DATASET_ROOT' in os.environ\n",
    "miniArticlesFilepath= os.path.join(os.environ['TAI_DATASET_ROOT'],'rag_ai_tutor','mini-llama-articles.txt')\n",
    "log.info(f'miniArticlesFilepath: {miniArticlesFilepath}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a643cc4-c050-49d0-8766-1bd4dd8d5ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert os.path.exists(  miniArticlesFilepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e623ad9-95e0-47e8-8ff8-cbb1f463580b",
   "metadata": {},
   "source": [
    "## Loads the mini articles in one string: text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c59259f-ce58-4988-a0c1-0b658c866ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171044\n"
     ]
    }
   ],
   "source": [
    "text= str()\n",
    "with open( miniArticlesFilepath, mode=\"r\", encoding=\"utf-8\") as file:\n",
    "    csvReader= csv.reader(file)\n",
    "\n",
    "    for idx, row in enumerate( csvReader):\n",
    "        if idx == 0:\n",
    "            continue\n",
    "        text += row[1]\n",
    "\n",
    "# The number of characters in the dataset.\n",
    "print( len( text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f13ed41-c9f9-42f5-b237-a50c0410e3d5",
   "metadata": {},
   "source": [
    "### Divides the text into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb5c79f6-6e45-4fbf-89d4-37b28f36d4ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"LLM Variants and Meta's Open Source Before shedding light on four major trends, I'd share the latest Meta's Llama 2 and Code Lla\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0: 128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e21b22a-5944-4962-93ea-5440d6adade0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ode Large Language Models with Evol-Instruct'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[171000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b1578a9-036b-4de3-b3c1-1d36f8f1b936",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ INFO] 14:43:11: numChunks:335\n"
     ]
    }
   ],
   "source": [
    "chunkSize= 512\n",
    "chunks= []\n",
    "for i in range( 0, len(text), chunkSize):\n",
    "    chunks.append( text[ i: i+chunkSize])\n",
    "\n",
    "numChunks= len(chunks)    \n",
    "log.info(f'numChunks:{numChunks}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ca6a549-a72a-4fea-a9a2-ecac02402756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512,\n",
       " \"ing, including InstructCodeT5+, StarCoder-GPTeacher, and Instruct-Codegen-16B. In conclusion, WizardCoder's success is attributed to its unique dataset and the innovative use of Evol-Instruct to enhance instruction complexity, leading to its outstanding performance across various code-related tasks and benchmarks.  References YouTube: WizardCoder 34B: Complex Fine-Tuning Explained GitHub Paper: WizardLM- Empowering Large Language Models to Follow Complex Instructions Paper: WizardCoder: Empowering Code Larg\")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "( len(chunks[333]),chunks[333])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7c246b3-f737-4a59-9205-ffa7784cb300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36, 'e Language Models with Evol-Instruct')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "( len(chunks[334]),chunks[334])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfefd49c-4372-4b82-b433-d51e1148ac21",
   "metadata": {},
   "source": [
    "## Using Chroma DB with LlamaIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88ff862b-0888-49b5-a742-3436456840f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.gemini import Gemini\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "248aeece-ed46-4a90-a8d9-4b91c4dff4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the text chunks into Documents\n",
    "documents= [ Document( text=t) for t in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0548957a-0231-4050-8203-5c34b6a28944",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ INFO] 14:43:13: Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    }
   ],
   "source": [
    "ChromaDbPath= './mini-dataset'\n",
    "if os.path.exists(ChromaDbPath):\n",
    "    print(f'The path {ChromaDbPath} exists')\n",
    "    shutil.rmtree(ChromaDbPath, ignore_errors=True)\n",
    "    \n",
    "# Creates the persistent client (i.e. not in memory).\n",
    "chromaClient=     chromadb.PersistentClient(path=ChromaDbPath)\n",
    "chromaCollection= chromaClient.create_collection(\"MiniDataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e0b102f-9924-4c7c-ac2d-9b576309b7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorStore =    ChromaVectorStore(chroma_collection=chromaCollection)\n",
    "storageContext = StorageContext.from_defaults(vector_store=vectorStore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35a2b4ca-20a9-4227-8c05-091a96dc6961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16c23efc588d401b9e2823fd0cd10c34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/335 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "039c9fc8f2974d678180822a877e427f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/335 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ INFO] 14:43:20: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "[ INFO] 14:43:22: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "[ INFO] 14:43:24: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "[ INFO] 14:43:25: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    embed_model=OpenAIEmbedding(model=\"text-embedding-3-small\"),\n",
    "    storage_context=storageContext,\n",
    "    show_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83ff9be5-3c0c-452f-b50d-2fc8cfe80602",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Gemini(model=\"models/gemini-1.5-flash\", temperature=1, max_tokens=512)\n",
    "queryEngine = index.as_query_engine(llm=llm, similarity_top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "387caff8-2f56-491e-a5cb-2ad4ba08c6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ INFO] 14:43:28: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Llama 2 model comes in four sizes: 7 billion, 13 billion, 34 billion, and 70 billion parameters.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = queryEngine.query(\"How many parameters LLaMA2 model has?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522a4bc9-52ec-487f-b071-98ce4a4a530e",
   "metadata": {},
   "source": [
    "## Using Chroma DB with LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "261dee66-75f8-4956-a500-ad927a8caa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.schema.document import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "323340b1-f413-4ff3-b6a5-674cc1ec7054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Chunks:335\n",
      "Number of documents:335\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of Chunks:{ len(chunks)}')\n",
    "documents = [Document(page_content=t) for t in chunks]\n",
    "print(f'Number of documents:{ len(documents)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "17ae194b-e62c-4ea3-94c5-306632b0dc83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The path ./mini-chunked-dataset does not exist.\n"
     ]
    }
   ],
   "source": [
    "LcChromaDbPath= './mini-chunked-dataset'\n",
    "LcCollectionName= 'mini-chunked-dataset'\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "if os.path.exists(LcChromaDbPath):\n",
    "    print(f'The path {LcChromaDbPath} exists.')    \n",
    "    shutil.rmtree(LcChromaDbPath)\n",
    "else:\n",
    "    print(f'The path {LcChromaDbPath} does not exist.')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c639c1b0-8e49-4a36-9a7e-d325698a76a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ INFO] 14:49:22: Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "[ INFO] 14:49:31: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "lcChromaDb = Chroma.from_documents( \n",
    "    documents=documents,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=LcChromaDbPath,\n",
    "    collection_name=LcCollectionName,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e0cca1b1-7773-4876-b98d-5304598b7199",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=512,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6c458af8-3eb9-4435-a74f-b6026ba7829b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ INFO] 14:51:51: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Llama 2 model is available in four different sizes: 7 billion, 13 billion, 34 billion, and 70 billion parameters.\n"
     ]
    }
   ],
   "source": [
    "query = \"How many parameters LLaMA2 model has?\"\n",
    "\n",
    "retriever = lcChromaDb.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "# Define a RetrievalQA chain that is responsible for retrieving related pieces of text,\n",
    "# and using a LLM to formulate the final answer.\n",
    "chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
    "\n",
    "response = chain.invoke(query)\n",
    "print(response[\"result\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
